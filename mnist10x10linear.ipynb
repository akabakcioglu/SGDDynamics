{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Statistics, LinearAlgebra, Base.Iterators, Random, StatsBase\n",
    "ENV[\"COLUMNS\"] = 80\n",
    "ARRAY = KnetArray{Float64}\n",
    "XSIZE=100   # input dimension\n",
    "YSIZE=10    # output dimension\n",
    "BATCHSIZE=100 # minibatch size\n",
    "LAMBDA=1e-2 # regularization parameter\n",
    "LR=1e-1     # learning rate\n",
    "MITER=5000  # iterations for finding minimum\n",
    "DITER=10^5  # iterations for diffusion tensor\n",
    "CITER=10^6  # iterations for covariance trajectory \n",
    "CFREQ=10^1  # keep every CFREQ points on trajectory\n",
    "@show gpu();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define regularized linear model with softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(w,x) = reshape(w,YSIZE,XSIZE) * reshape(x,XSIZE,:)\n",
    "loss(w,x,y;λ=LAMBDA) = nll(pred(w,x), y) + (λ/2) * sum(abs2,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrn,ytrn,xtst,ytst = Knet.load(\"mnist10x10.jld2\",\"xtrn\",\"ytrn\",\"xtst\",\"ytst\")\n",
    "atrn,atst = ARRAY(xtrn), ARRAY(xtst) # GPU copies for batch training\n",
    "println.(summary.((xtrn,ytrn,xtst,ytst,atrn,atst)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatching for SGD-I, i.e. with replacement. Knet.minibatch can't do this, we define new struct\n",
    "struct MB; x; y; n; end\n",
    "Base.Iterators.IteratorSize(::Type{MB}) = Base.IsInfinite() # need this for collect to work\n",
    "Base.iterate(d::MB, s...)=(r = rand(1:length(d.y),d.n); ((ARRAY(mat(d.x)[:,r]), d.y[r]), true))\n",
    "dtrn = MB(xtrn, ytrn, BATCHSIZE)\n",
    "println.(summary.(first(dtrn)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,MITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum without minibatching\n",
    "# ~50 iters/sec, LAMBDA=1e-2, MITER=5000, converges in 1 min to \n",
    "# (trnloss = 0.88162015413913, nll = 0.6166511380346865, reg = 0.2649690161044434)\n",
    "wminfile = \"mnist10x10linear-wmin-$LAMBDA-$MITER.jld2\"\n",
    "if !isfile(wminfile)\n",
    "    wmin = Param(ARRAY(zeros(XSIZE*YSIZE)))\n",
    "    args = repeat([(wmin,atrn,ytrn)],MITER)\n",
    "    Knet.gc()\n",
    "    losses = collect(progress(adam(loss,args)))\n",
    "    Knet.save(wminfile, \"wmin\", wmin, \"losses\", losses)\n",
    "else\n",
    "    wmin, losses = Knet.load(wminfile, \"wmin\", \"losses\");\n",
    "end\n",
    "@show summary(wmin)\n",
    "losses[1000:1000:end]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println.((\n",
    "(trnloss=loss(wmin,atrn,ytrn),nll=nll(pred(wmin,atrn),ytrn),reg=(LAMBDA/2)*sum(abs2,wmin)),\n",
    "(tstloss=loss(wmin,atst,ytst),nll=nll(pred(wmin,atst),ytst),reg=(LAMBDA/2)*sum(abs2,wmin)),\n",
    "(trnacc=accuracy(pred(wmin,atrn),ytrn),tstacc=accuracy(pred(wmin,atst),ytst))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian of loss around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessian(loss,w,x,y)\n",
    "    ∇loss = grad(loss)\n",
    "    ∇lossi(w,x,y,i) = ∇loss(w,x,y)[i]\n",
    "    ∇∇lossi = grad(∇lossi)\n",
    "    w = value(w)\n",
    "    n = length(w)\n",
    "    h = similar(Array(w),n,n)\n",
    "    for i in progress(1:n)\n",
    "        h[:,i] .= Array(vec(∇∇lossi(w,x,y,i)))\n",
    "    end\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hessian: ~15 secs\n",
    "hessfile = \"mnist10x10linear-hess-$LAMBDA.jld2\"\n",
    "if !isfile(hessfile)\n",
    "    Knet.gc()\n",
    "    H = hessian(loss,wmin,atrn,ytrn)\n",
    "    Knet.save(hessfile,\"h\",H)\n",
    "else\n",
    "    H = Knet.load(hessfile,\"h\")\n",
    "end\n",
    "summarystats(vec(H)) |> dump\n",
    "@show norm(H), norm(H-H');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heigfile = \"mnist10x10linear-heig-$LAMBDA.jld2\"\n",
    "if !isfile(heigfile)\n",
    "    @time eigenH = eigen(Symmetric(H)) # ~1s\n",
    "    Knet.save(heigfile,\"eigenH\",eigenH)\n",
    "else\n",
    "    eigenH = Knet.load(heigfile,\"eigenH\")\n",
    "end\n",
    "eigenH.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(eigenH.values) |> dump\n",
    "plot(reverse(eigenH.values), yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(loss,w,x,y;iters=DITER,lr=LR,batchsize=BATCHSIZE)\n",
    "    ∇loss = grad(loss)\n",
    "    grad0 = Array(∇loss(w, ARRAY(x), y))\n",
    "    data = MB(x,y,batchsize)\n",
    "    grads = zeros(length(w), iters)\n",
    "    for (i,d) in progress(enumerate(take(data,iters)))\n",
    "        grads[:,i] .= Array(∇loss(w,d...))\n",
    "    end\n",
    "    prefac = (lr^2)/(2iters)\n",
    "    grads = grad0 .- grads\n",
    "    @time v = prefac * (grads * grads')\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,LR,BATCHSIZE,DITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtfile = \"mnist10x10linear-diff-$LAMBDA-$LR-$BATCHSIZE-$DITER.jld2\"\n",
    "if !isfile(dtfile)\n",
    "    Knet.gc()\n",
    "    D = diffusiontensor(loss,wmin,xtrn,ytrn) # ~1600 iters/sec, 60secs total\n",
    "    Knet.save(dtfile,\"D\",D)\n",
    "else\n",
    "    D = Knet.load(dtfile,\"D\")\n",
    "end\n",
    "summarystats(vec(D)) |> dump\n",
    "@show norm(D),norm(D-D');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the diffusion tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deigfile = \"mnist10x10linear-deig-$LAMBDA-$LR-$BATCHSIZE-$DITER.jld2\"\n",
    "if !isfile(deigfile)\n",
    "    @time eigenD = eigen(Symmetric(D)) # ~1s\n",
    "    Knet.save(deigfile,\"eigenD\",eigenD)\n",
    "else\n",
    "    eigenD = Knet.load(deigfile,\"eigenD\")\n",
    "end\n",
    "eigenD.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(eigenD.values) |> dump\n",
    "plot(reverse(eigenD.values) .+ 1e-20, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record trajectory with SGD starting at minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,LR,BATCHSIZE,CITER,CFREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory of w starting from wmin recorded after each update: \n",
    "# ~1800 updates/sec, ~9 mins total for CITER=1M\n",
    "trajfile = \"mnist10x10linear-traj-$LAMBDA-$LR-$BATCHSIZE-$CITER-$CFREQ.jld2\"\n",
    "if !isfile(trajfile)\n",
    "    w = Param(ARRAY(value(wmin)))\n",
    "    data = MB(xtrn,ytrn,BATCHSIZE)\n",
    "    d = take(data,CITER)\n",
    "    W = zeros(eltype(w),length(w),div(CITER,CFREQ))\n",
    "    f(x,y) = loss(w,x,y)\n",
    "    Knet.gc()\n",
    "    i = 0\n",
    "    for t in progress(sgd(f,d; lr=LR))\n",
    "        i += 1; (div,rem)=divrem(i,CFREQ)\n",
    "        if rem == 0\n",
    "            W[:,div] = Array(vec(w))\n",
    "        end\n",
    "    end\n",
    "    Knet.save(trajfile,\"W\",W)\n",
    "else\n",
    "    W = Knet.load(trajfile,\"W\")\n",
    "end\n",
    "summary(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot losses on whole dataset, first steps seem transient, ~6 secs\n",
    "r = 1:100:size(W,2)\n",
    "@time plot(r, [loss(ARRAY(W[:,i]),atrn,ytrn) for i in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory of two random dimensions\n",
    "@show i1,i2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "if norm(W[i1,:]) > 0 && norm(W[i2,:]) > 0\n",
    "    histogram2d(W[i1,:],W[i2,:])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training seems to converge to a slightly worse spot, but mean is close to optimal\n",
    "w0 = Array(value(wmin))\n",
    "μ = mean(W[:,2500:end],dims=2)\n",
    "w1 = W[:,end]\n",
    "@show norm(w0), norm(μ), norm(w0 - μ)\n",
    "@show extrema(w0), extrema(μ), extrema(w0 - μ)\n",
    "@show mean(abs.(w0 - μ) .> 0.01)\n",
    "@show loss(w0,xtrn,ytrn)\n",
    "@show loss(μ,xtrn,ytrn)\n",
    "@show loss(w1,xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance of SGD trajectory around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wstable = W[:,2500:end];  @show summary(Wstable)\n",
    "Wstable = W\n",
    "μ = mean(Wstable,dims=2); @show summary(μ)\n",
    "Wzero = Wstable .- μ;     @show summary(Wzero)\n",
    "@time Σ = (Wzero * Wzero') / size(Wzero,2) # ~1s\n",
    "summarystats(vec(Σ)) |> dump\n",
    "@show norm(Σ),norm(Σ-Σ');\n",
    "@show summary(Σ)\n",
    "@show norm(Σ)\n",
    "@show extrema(Σ)\n",
    "@show norm(diag(Σ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for convergence\n",
    "n2 = div(size(W,2),2)\n",
    "w1 = W[:,1:n2]\n",
    "w2 = W[:,1+n2:end]\n",
    "w1 = w1 .- mean(w1,dims=2)\n",
    "w2 = w2 .- mean(w2,dims=2)\n",
    "Σ1 = (w1 * w1') / size(w1,2)\n",
    "Σ2 = (w2 * w2') / size(w2,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variances (diagonal elements) converge\n",
    "norm(diag(Σ1)),norm(diag(Σ2)),norm(diag(Σ1)-diag(Σ2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The off diagonal elements are still not there\n",
    "norm(Σ1),norm(Σ2),norm(Σ1-Σ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceigfile = \"mnist10x10linear-ceig-$LAMBDA-$LR-$BATCHSIZE-$CITER-$CFREQ.jld2\"\n",
    "if !isfile(ceigfile)\n",
    "    @time eigenC = eigen(Symmetric(Σ)) # ~1s\n",
    "    Knet.save(ceigfile,\"eigenC\",eigenC)\n",
    "else\n",
    "    eigenC = Knet.load(ceigfile,\"eigenC\")\n",
    "end\n",
    "eigenC.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: no need for log-scale in plot this time\n",
    "summarystats(eigenC.values) |> dump\n",
    "plot(reverse(eigenC.values)) # .+ 1e-19, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.((H,D,Σ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = H*Σ + Σ*H\n",
    "b = (2/LR)*D\n",
    "norm(a),norm(b),norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for Sigma as in Mike's notes\n",
    "function fsigma(eigenH,D)\n",
    "    O = eigenH.vectors\n",
    "    ODO = O'*D*O;\n",
    "    n = size(D,1)\n",
    "    Delta=zero(D)\n",
    "    for i=1:n\n",
    "        for j=1:n\n",
    "            Delta[i,j]=ODO[i,j]/(eigenH.values[i]+eigenH.values[j])\n",
    "        end\n",
    "    end\n",
    "    Sigma = O*Delta*O'\n",
    "    return Sigma\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = fsigma(eigenH,D)\n",
    "norm(Σ),norm(S),norm(Σ-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dΣ,dS = diag(Σ),diag(S)\n",
    "norm(dΣ),norm(dS),norm(dΣ-dS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(dΣ ./ dS) |> dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S20 = 20*fsigma(eigenH,D)\n",
    "norm(Σ),norm(S20),norm(Σ-S20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dΣ,dS20 = diag(Σ),diag(S20)\n",
    "norm(dΣ),norm(dS20),norm(dΣ-dS20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S == S', Σ == Σ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(S),norm(S-S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(diag(S20)), mean(S20-Diagonal(S20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(diag(Σ)), mean(Σ-Diagonal(Σ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUNK below this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm(a),norm(b),norm(a-b)\n",
    "# (0.001831031218512692, 0.0015956482650563955, 0.000668750876334433): CITER=1M CFREQ=100\n",
    "# (0.0017456054525856169, 0.0015956482650563955, 0.00044092592553571534): CITER=1M CFREQ=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[3000:3004,3000:3004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[3000:3004,3000:3004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a./b)[3000:3004,3000:3004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(vec(abs.(a))) |> dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = a; a0[abs.(a0) .< 1e-7] .= 0\n",
    "b0 = b; b0[abs.(b0) .< 1e-7] .= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(a0), norm(b0), norm(a0-b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check convergence of covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigma(W)\n",
    "    μ = mean(W,dims=2)\n",
    "    W0 = W .- μ\n",
    "    Σ = (W0 * W0') / size(W0,2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [ sigma(W[:,1:(i*1000)]) for i in (1,2,5,10,20,50,100) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 2:length(sigmas)\n",
    "   println((norm(sigmas[i-1]),norm(sigmas[i]),norm(sigmas[i]-sigmas[i-1])))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1K,2K,...,10K sampling every 100 up to 1M convergence\n",
    "(0.17517119848767196, 0.18735112177809712, 0.1736369823475756)\n",
    "(0.18735112177809712, 0.18194022797202322, 0.13163000956991822)\n",
    "(0.18194022797202322, 0.17754343048529778, 0.10970021736453625)\n",
    "(0.17754343048529778, 0.16902882189691706, 0.08681261945879391)\n",
    "(0.16902882189691706, 0.16118998631852746, 0.07373054738895173)\n",
    "(0.16118998631852746, 0.15395757281329958, 0.06383283177104303)\n",
    "(0.15395757281329958, 0.14673395708896458, 0.05427855979560519)\n",
    "(0.14673395708896458, 0.14047011468580411, 0.04757881795972907)\n",
    "(0.14047011468580411, 0.13584321137382668, 0.04594090264182607)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1K,2K,5K,10K,20K,50K,100K sampling every 10 up to 1M \n",
    "(0.09419547449556358, 0.12122309088637423, 0.11268632188794234)\n",
    "(0.12122309088637423, 0.15926002834426348, 0.15786197264298812)\n",
    "(0.15926002834426348, 0.17998457741465182, 0.16622733785950075)\n",
    "(0.17998457741465182, 0.18679671394160588, 0.1732145346986742)\n",
    "(0.18679671394160588, 0.16400614726671942, 0.17491052732529933)\n",
    "(0.16400614726671942, 0.1335983636197007, 0.1283291677336581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 2:length(sigmas)\n",
    "   println((norm(diag(sigmas[i-1])),norm(diag(sigmas[i])),norm(diag(sigmas[i]-sigmas[i-1]))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(norm.(diag.(sigmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sort(diag(H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian (numeric check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(w) ≈ f(wmin) + (w-wmin)' g + 1/2 (w-wmin)' H (w-wmin)\n",
    "# Gradient at wmin is ≈0, so the middle term can be assumed 0\n",
    "df = @diff loss(wmin,atrn,ytrn)\n",
    "J = vec(grad(df, wmin)); @show summary(J)\n",
    "@show norm(J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test approx at ~1 distance around wmin\n",
    "# adding first order term does not make much difference as expected\n",
    "wrnd = randn!(similar(wmin)) / sqrt(length(wmin))\n",
    "lossw(w) = loss(w,atrn,ytrn)\n",
    "@show lossw(wmin)\n",
    "@show lossw(wmin + wrnd)\n",
    "@show lossw(wmin) + 0.5 * wrnd' * hmin * wrnd\n",
    "@show lossw(wmin) + J' * wrnd + 0.5 * wrnd' * hmin * wrnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check convergence of variance in SGD trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE,LR,LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w  = ARRAY(value(wmin))\n",
    "w1 = w .+ 0\n",
    "w2 = w .* w\n",
    "nw = 1\n",
    "pw = Param(w)\n",
    "data = MB(xtrn,ytrn,BATCHSIZE)\n",
    "f(x,y) = loss(pw,x,y,\\lambda=LAMBDA)\n",
    "\n",
    "function wvar(w)\n",
    "    global nw, w1, w2\n",
    "    nw += 1\n",
    "    w1 .+= w\n",
    "    w2 .+= w .* w\n",
    "    var = w2 / nw - (w1 .* w1) / (nw * nw) # E[x^2] - E[x]^2\n",
    "    norm(var)\n",
    "end\n",
    "\n",
    "c = collect(progress(wvar(w) for wloss in sgd(f, take(data,500000), lr=LR)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(c, xscale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(w),norm(w.*w),norm(w1),norm(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionTensor Convergence \n",
    "(with LAMBDA=0.0001,LR=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm(d100),norm(d1000),norm(d100-d1000)         #(9.006901905269366e-5, 7.966566524654371e-5, 4.746245825247884e-5)\n",
    "#norm(d1000),norm(d2000),norm(d1000-d2000)       #(7.966566524654371e-5, 7.976183314696048e-5, 1.8908596356951573e-5)\n",
    "#norm(d4000),norm(d2000),norm(d4000-d2000)       #(8.024754500933944e-5, 7.976183314696048e-5, 1.3446098748867312e-5)\n",
    "#norm(d10000),norm(d4000),norm(d10000-d4000)     #(7.942869188760434e-5, 8.024754500933944e-5, 8.963487531775732e-6)\n",
    "#norm(d10000),norm(d20000),norm(d20000-d10000)   #(7.955107659141219e-5, 7.976461525637195e-5, 5.902290704618786e-6)\n",
    "#norm(d20000),norm(d50000),norm(d50000-d20000)   #(7.976461525637195e-5, 7.971793474706985e-5, 3.962945418293533e-6)\n",
    "#norm(d50000),norm(d100000),norm(d100000-d50000) #(7.971793474706985e-5, 7.978241325281978e-5, 2.739184868054573e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plots for trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to converge to a slightly different point?\n",
    "# Interesting patterns: staircase, globe, H shaped\n",
    "@show r1,r2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "scatter(W[r1,1:end],W[r2,1:end])\n",
    "scatter!(W[r1,1:10], W[r2,1:10],mc=:red) # mark beginning with red\n",
    "scatter!(W[r1,end-9:end],W[r2,end-9:end],mc=:yellow) # mark end with yellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing covariance with fit_mle fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = fit_mle(MvNormal, Wstable)\n",
    "# Σ is not positive definite, MLE fails because Hessian=inverse(Σ)\n",
    "# Note that this is not the same as the loss Hessian defined below, it is the distribution Hessian!\n",
    "# using Distributions\n",
    "@show sum(diag(Σ) .== 0) # 670 dims do not move at all!\n",
    "@show sum(wmin .== 0) # these stay at 0 in the w matrix.\n",
    "findall(diag(Σ) .== 0) == findall(Array(wmin) .== 0) # to make sure they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
