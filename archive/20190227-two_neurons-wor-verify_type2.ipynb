{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,Printf,GZip,Knet,Plots,LinearAlgebra,Distributions #,Interact,StatsBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating samples from a noisy Gaussian (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV[\"GRDIR\"]=\"\"\n",
    "# Pkg.build(\"GR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(4);\n",
    "Range=3.0; # range of the x values for the target Gaussian function\n",
    "Incr = 0.03; # determines the number of samples from which we'll learn\n",
    "Noise_std=0.1; # add noise on the Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data from which we'll learn the Gaussian function\n",
    "# obligatory arguments listed before \";\" while optional arguments come after \";\".\n",
    "function gen_noisy_gaussian(;range=1.0,noise=0.1)\n",
    "    x = collect(-Range:Incr:Range)\n",
    "    y = exp.(-x.^2) + randn(length(x))*noise; # additive gaussian noise\n",
    "    return (x,y)\n",
    "end\n",
    "# output is two vectors x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(range=Range,noise=Noise_std);\n",
    "pop!(x_train);pop!(y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain =length(x_train) # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network, the loss function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenSize = 2; # number of neurons in the hidden layer\n",
    "Batchsize = 10;\n",
    "RegWeight=0.001; # lambda for L2 regularization\n",
    "InitNorm = 0.5; # initial weight norm\n",
    "\n",
    "LearningRate = 0.01; # Ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deep learning package requires a certain array structure for the weights\n",
    "# but it is easier for the later analysis to dump them all into a single column vector\n",
    "function flat(w) # make a single vector out of all weights\n",
    "    return vcat(w[1],w[2],w[3],w[4])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the weight array from the flat weight vector\n",
    "function unflat(wf)\n",
    "    return [wf[1:HiddenSize],wf[HiddenSize+1:2*HiddenSize],wf[2*HiddenSize+1:3*HiddenSize],wf[end]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer network performing: tanh.(w[hidden,input] * x[input,batchsize] .+ b[hidden,1])\n",
    "# The dot \".\" is for \"broadcasting\": performing the operation pointwise on each input element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this seed to try different initial weigths w/o changing the training data\n",
    "Random.seed!(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights: w = [w1,w2,w3,w4] -> output = w3*tanh.(w1*x .+ w2) .+ w4\n",
    "w = [rand(HiddenSize),rand(HiddenSize),rand(HiddenSize),rand()];\n",
    "w = InitNorm*w/norm(flat(w)); # rescale w so that the norm is InitNorm\n",
    "Nweights = length(flat(w));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both x and y are ordered in columns per training data point\n",
    "function sqloss(w,x,y)\n",
    "    guesses =  w[3]'*tanh.(w[1]*x' .+ w[2]) .+ w[4];\n",
    "    return mean(abs2,y'-guesses)\n",
    "end\n",
    "\n",
    "function reg(w)\n",
    "    return RegWeight*sum(norm(w[i])^2  for i=1:4)\n",
    "end\n",
    "\n",
    "function loss(w,x,y)\n",
    "    return sqloss(w,x,y) + reg(w)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct gradient-calculating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grad() is a \"functional\" whose input and output\n",
    "# is a function. Note that grad() requires loss to be a scalar function\n",
    "lossgradient = grad(loss)\n",
    "sqlossgradient = grad(sqloss)\n",
    "reggradient = grad(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient at the initial w\n",
    "# dw has dimensions of w: each weight w_i is replaced with the gradient wrt w_i\n",
    "dw = lossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective loss function for type-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function eloss(w,x,y;lr=LearningRate)\n",
    "    n = floor(Int,Ntrain/Batchsize);\n",
    "    losslead = n*loss(w,x,y)\n",
    "    losscorr = norm(flat(lossgradient(w,x,y)))^2\n",
    "    losscorr += norm(flat(sqlossgradient(w,x,y)))^2/(Ntrain-1)\n",
    "    losscorr -= sum(norm(flat(sqlossgradient(w,[x[i]],[y[i]])))^2 for i=1:Ntrain)/(Ntrain*(Ntrain-1))\n",
    "    losscorr *= 0.25*lr*n*(n-1)\n",
    "    return (losslead - losscorr)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elossgradient=grad(eloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eloss(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "elossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING FUNCTION THAT PRESENTS THE TRAINING SET IN RANDOM BATCHES (WITH NO REPLACEMENT)\n",
    "# For random batches WITH replacement, move the line \"perm = ..\" inside the for loop\n",
    "\n",
    "function mytrain!(w;lr=0.1)\n",
    "    Nbatch = floor(Int,Ntrain/Batchsize); # few training samples will be left out if Ntrain/Batchsize != integer\n",
    "    perm = randperm(Ntrain); # a random permutation of [1:Ntrain] - pick batches as chunks from this array\n",
    "    for nb=0:Nbatch-1\n",
    "        x = [x_train[n] for n in perm[nb*Batchsize+1:(nb+1)*Batchsize]]\n",
    "        y = [y_train[n] for n in perm[nb*Batchsize+1:(nb+1)*Batchsize]]\n",
    "\n",
    "        # calculate gradient over the batch and update\n",
    "        dw = lossgradient(w,x,y);\n",
    "        for i=1:length(w)\n",
    "                w[i] -= lr*dw[i]\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 1000; # For a quick training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Collect weights after each epoch in an array (trajectory)\n",
    "@time w_training = [ deepcopy(mytrain!(w,lr=LearningRate)) for epoch=1:Nepoch ];  # copy only copies the top layer, does not descend.\n",
    "wf_training = zeros(Nepoch,Nweights); for i=1:Nepoch wf_training[i,:] = flat(w_training[i]) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare the learned function with the actual gaussian\n",
    "xplot=collect(-Range:0.01:Range) # create an array of x values within the range\n",
    "y_pred = w[3]'*tanh.(w[1]*xplot' .+ w[2]) .+ w[4] # generate the predicted y values\n",
    "# plot the converged function, the initial gaussian and the noisy training samples\n",
    "plot(xplot,[y_pred',exp.(-xplot.^2)]); scatter!(x_train,y_train,leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs epoch\n",
    "SamplingRate=10;\n",
    "x = collect(1:SamplingRate:Nepoch);\n",
    "y = [loss(w_training[i],x_train,y_train) for i in x];\n",
    "plot(x,y)\n",
    "#plot(x,y,xaxis=:log10,yaxis=:log10) # can also plot in log-log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion tensor, Hessian, Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the diffusion tensor by sampling the noise\n",
    "function diffusiontensor_num(w,n;lr=LearningRate) # n: number of samples used for estimation\n",
    "    wstart = deepcopy(w)\n",
    "    wlist = zeros(Nweights,n)\n",
    "    \n",
    "    for i=1:n\n",
    "        ww = deepcopy(wstart)\n",
    "        mytrain!(ww,lr=LearningRate)\n",
    "        wlist[:,i] = flat(ww)\n",
    "    end\n",
    "\n",
    "    # subtract mean\n",
    "    wlist .-= sum(wlist[:,i] for i=1:n)/n\n",
    "\n",
    "    D = zeros(Nweights,Nweights)\n",
    "    for α=1:Nweights\n",
    "        for β=1:Nweights\n",
    "            for i=1:n\n",
    "                D[α,β] += wlist[α,i]*wlist[β,i]\n",
    "            end\n",
    "            D[α,β] /= n;\n",
    "        end\n",
    "    end\n",
    "    return 0.5*D\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the Hessian,\n",
    "# define a function returning elements of the loss-gradient vector dL/dw_j\n",
    "\n",
    "function lossgradj(w,x,y,j)\n",
    "    return flat(lossgradient(w,x,y))[j]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function calculating a column of Hessian matrix:\n",
    "# Returns d^2(L)/dw_idw_j for all i and given j\n",
    "lossgradgrad = grad(lossgradj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessianmatrix(w,x,y)\n",
    "    Nw = length(flat(w));\n",
    "    Hess = zeros(Nw,Nw);\n",
    "    for j=1:Nw\n",
    "        Hess[:,j] = flat(lossgradgrad(w,x,y,j))\n",
    "        end\n",
    "    return Hess\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculation is from Michael's overleaf notes:\n",
    "# https://www.overleaf.com/2523873322bvvnxpwnskfk\n",
    "function covariancematrix(D,H;lr=LearningRate)\n",
    "    F = eigen(H);\n",
    "    h = F.values\n",
    "    O = F.vectors\n",
    "    Nw = length(h)\n",
    "    ODO = O'*D*O;\n",
    "    Delta = zeros(Nw,Nw);\n",
    "    for i=1:Nw\n",
    "        for j=1:Nw\n",
    "            Delta[i,j] = ODO[i,j]/(h[i]+h[j])\n",
    "        end\n",
    "    end\n",
    "    return (2/lr)*O*Delta*O'\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Newton's Method to find the minimum of loss (not eloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Newton's method get the the true minimum of the full loss function\n",
    "wf = flat(w_training[end]);\n",
    "# implement Newton's method to find the true minimum\n",
    "for n=1:10\n",
    "    Hess = hessianmatrix(unflat(wf),x_train,y_train)\n",
    "    gradwf = flat(lossgradient(unflat(wf),x_train,y_train))\n",
    "    wf = wf - inv(Hess)*gradwf\n",
    "end\n",
    "\n",
    "wminf = wf[:,1]\n",
    "wmin = unflat(wminf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossgradient(wmin,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian at the loss minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hessian at the loss minimum\n",
    "Hessmin = hessianmatrix(wmin,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion tensor at the loss minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion tensor at the loss minimum (not the minimum of effective loss)\n",
    "Dmin = diffusiontensor_num(wmin,10000;lr=LearningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix as a function of Hessian and Diffusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Covmin = covariancematrix(Dmin,Hessmin,lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: must be zero\n",
    "Hessmin*Covmin + Covmin*Hessmin - (2/LearningRate)*Dmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steady-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(2) # Verified that the results don't change for different seeds.\n",
    "Nepoch2 = 100000; # will take about 10 min for 100,000 epochs\n",
    "LearningRate = 0.001;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = deepcopy(wmin); # start from the minimum of the potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@time w_ss = [ deepcopy(mytrain!(w,lr=LearningRate)) for epoch=1:Nepoch2 ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the flat trajectory\n",
    "wf_ss = zeros(Nepoch2,Nweights); for i=1:Nepoch2 wf_ss[i,:] = flat(w_ss[i]) end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the steady-state distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index = 5\n",
    "y_index = 6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "trans=30000 # earlier points are transient from loss minimum to the \"effective loss\" minimum\n",
    "hrange=collect(trans:length(wf_ss[:,1]))\n",
    "resxy=(200,200)\n",
    "fith = fit(Histogram,(wf_ss[hrange,x_index],wf_ss[hrange,y_index]),nbins=resxy)\n",
    "fith.weights\n",
    "fith.edges\n",
    "maxhist=maximum(fith.weights)\n",
    "#heatmap(fith.weights)\n",
    "#histogram2d(wf_ss[1:end,x_index],wf_ss[1:end,y_index],bins=resxy)\n",
    "histogram2d(wf_ss[trans:end,x_index],wf_ss[trans:end,y_index],bins=resxy)\n",
    "scatter!([wminf[x_index,1]],[wminf[y_index,1]],leg=false,markercolor=\"cyan\",markersize=4) # loss minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Mv-Gaussian to the equilibrium data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Fit_ss = fit_mle(MvNormal,wf_ss[trans:end,:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steady-state mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanw = Distributions.mean(Fit_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_ss = Distributions.cov(Fit_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals(Cov_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_ss_inv = inv(Cov_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_xy_inv = inv(Cov_ss[[x_index,y_index],[x_index,y_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion tensor (numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion tensor at the minimum of effective loss out of 10,000 samples\n",
    "Dmin_ef = diffusiontensor_num(unflat(meanw),10000;lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gradient descend get to the minimum of the effective loss function\n",
    "# starting from the steady-state mean\n",
    "wf = copy(meanw);\n",
    "delta = 0.03;\n",
    "# implement GD method to find the true minimum\n",
    "for n=1:300\n",
    "    gradwf = flat(elossgradient(unflat(wf),x_train,y_train))\n",
    "    wf = wf - delta*gradwf # the prefactor needs to be chosen properly by trial-and-error\n",
    "end\n",
    "\n",
    "wmin_ef = wf[:,1]\n",
    "wmin_e = unflat(wmin_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elossgradient(wmin_e,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion tensor at the minimum of effective loss\n",
    "Dmin_ef = diffusiontensor_num(wmin_e,10000;lr=LearningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the steady-state distribution on top of the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a grid enclosing the steady-state trajectory\n",
    "Lx = (maximum(wf_ss[trans:end,x_index])-minimum(wf_ss[trans:end,x_index]))/2\n",
    "Ly = (maximum(wf_ss[trans:end,y_index])-minimum(wf_ss[trans:end,y_index]))/2\n",
    "xrange = 1.5*Lx\n",
    "yrange = 1.5*Ly\n",
    "Nx = Ny = 10\n",
    "dx = xrange/Nx\n",
    "dy = yrange/Ny\n",
    "xx = -xrange:dx:xrange;\n",
    "yy = -yrange:dy:yrange;\n",
    "x = collect(xx) .+ meanw[x_index]\n",
    "y = collect(yy) .+ meanw[y_index]\n",
    "Identity = Diagonal(ones(Nweights,Nweights));\n",
    "Imask = Identity; Imask[x_index,x_index]=0;Imask[y_index,y_index]=0;\n",
    "xmask = zeros(Nweights); xmask[x_index]=1.0;\n",
    "ymask = zeros(Nweights); ymask[y_index]=1.0;\n",
    "\n",
    "histogram2d(wf_ss[trans:end,x_index],wf_ss[trans:end,y_index],bins=200)\n",
    "\n",
    "fexp(xi,yi) = -(([xi yi]-meanw[[x_index y_index]])*Cov_xy_inv*([xi yi]-meanw[[x_index y_index]])')[1]\n",
    "ffit(xi,yi) =  maxhist * fexp(xi,yi)/fexp(x[end],y[end])\n",
    "contour!(x,y,ffit,linestyle=:dash)\n",
    "\n",
    "## Actual loss contours\n",
    "#lossxy(x,y) = 5e9*(loss(unflat(Imask*meanw + x*xmask + y*ymask),x_train,y_train)-loss(unflat(meanw),x_train,y_train))\n",
    "#contour!(x,y,lossxy)\n",
    "fexp(xi,yi) = loss(unflat(Imask*meanw + xi*xmask + yi*ymask),x_train,y_train) - loss(wmin,x_train,y_train)\n",
    "flossxy(xi,yi) = maxhist * log(fexp(xi,yi))/log(fexp(x[Nx],y[Ny]))\n",
    "contour!(x,y,flossxy)\n",
    "\n",
    "## Effective loss contours - takes a while to compute since eloss() is not simple\n",
    "elossmin = eloss(wmin_e,x_train,y_train)\n",
    "fexp(xi,yi) = eloss(unflat(Imask*meanw + xi*xmask + yi*ymask),x_train,y_train) - elossmin\n",
    "elossxy(xi,yi) = maxhist * log(fexp(xi,yi))/log(fexp(x[Nx],y[Ny]))\n",
    "contour!(x,y,elossxy,linestyle=:dashdot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move to the eigen-coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most relevant directions\n",
    "Xidx = Nweights\n",
    "Yidx = Nweights-1\n",
    "\n",
    "O = eigvecs(Cov_ss);\n",
    "W_ss = wf_ss*O; # sample weights are row vectors\n",
    "COV_ss = O'*Cov_ss*O\n",
    "COV_xy_inv = inv(COV_ss[[Xidx,Yidx],[Xidx,Yidx]])\n",
    "\n",
    "meanW = O'*meanw\n",
    "Wmin_ef = O'*wmin_ef\n",
    "Wminf = O'*wminf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram2d(W_ss[trans:end,Xidx],W_ss[trans:end,Yidx],bins=200,aspect_ratio=1)\n",
    "\n",
    "Lx = (maximum(W_ss[trans:end,Xidx])-minimum(W_ss[trans:end,Xidx]))/2\n",
    "Ly = (maximum(W_ss[trans:end,Yidx])-minimum(W_ss[trans:end,Yidx]))/2\n",
    "xrange = 1.5*Lx\n",
    "yrange = 1.5*Ly\n",
    "Nx = Ny = 10\n",
    "dx = xrange/Nx\n",
    "dy = yrange/Ny\n",
    "xx = -xrange:dx:xrange;\n",
    "yy = -yrange:dy:yrange;\n",
    "x = collect(xx) .+ meanW[Xidx]\n",
    "y = collect(yy) .+ meanW[Yidx]\n",
    "Identity = Array(Diagonal(ones(Nweights,Nweights)));\n",
    "Imask = Identity; Imask[Xidx,Xidx]=0;Imask[Yidx,Yidx]=0;\n",
    "xmask = zeros(Nweights); xmask[Xidx]=1.0;\n",
    "ymask = zeros(Nweights); ymask[Yidx]=1.0;\n",
    "\n",
    "# Contours of the fit mv-Gaussian\n",
    "fexp(xi,yi) = -(([xi yi]-meanW[[Xidx Yidx]])*COV_xy_inv*([xi yi]-meanW[[Xidx Yidx]])')[1]\n",
    "Ffit(xi,yi) = maxhist* fexp(xi,yi)/fexp(x[end],y[end])\n",
    "#contour!(x,y,Ffit,linestyle=:dash)\n",
    "\n",
    "# contours of the original loss\n",
    "fexp(xi,yi) = loss(unflat(O*(Imask*meanW + xi*xmask + yi*ymask)),x_train,y_train) - loss(wmin,x_train,y_train)\n",
    "Flossxy(xi,yi) = (maxhist/5) * (log(fexp(xi,yi)) - log(fexp(x[Nx],y[Ny])))\n",
    "contour!(x,y,Flossxy)\n",
    "\n",
    "# contours of effective loss - takes a while since eloss() is not simple\n",
    "#elossmin = eloss(wmin_e,x_train,y_train)\n",
    "#fexp(xi,yi) = eloss(unflat(O*(Imask*meanW + xi*xmask + yi*ymask)),x_train,y_train) - elossmin\n",
    "#Felossxy(xi,yi) = (maxhist/2) * log(fexp(xi,yi))/log(fexp(x[Nx],y[Ny]))\n",
    "#contour!(x,y,Felossxy,linestyle=:dashdot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Mike's calculations for effective loss and diffusion matrix in type-II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test the effective loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual confirmation (minimum of effective loss is in the middle of the cloud)\n",
    "histogram2d(W_ss[trans:end,Xidx],W_ss[trans:end,Yidx],bins=200)#,aspect_ratio=1)\n",
    "\n",
    "scatter!([Wmin_ef[Xidx,1]],[Wmin_ef[Yidx,1]],leg=false,markercolor=\"white\",markersize=6)\n",
    "scatter!([meanW[Xidx]],[meanW[Yidx]],leg=false,markercolor=\"blue\",markersize=4)\n",
    "# scatter!([Wminf[Xidx]],[Wminf[Yidx]],leg=false,markercolor=\"yellow\",markersize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next test the diffusion tensor expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the theory for the diffusion matrix of type-II\n",
    "function sqlossgradj(w,x,y,j)\n",
    "    return flat(sqlossgradient(w,x,y))[j]\n",
    "end\n",
    "sqlossgradgrad = grad(sqlossgradj)\n",
    "\n",
    "function reggradj(w,j)\n",
    "    return flat(reggradient(w))[j]\n",
    "end\n",
    "reggradgrad = grad(reggradj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V matrix\n",
    "V = zeros(Nweights,Ntrain) # initialize the diffusion matrix\n",
    "for i=1:Ntrain\n",
    "    x=x_train[i]\n",
    "    y=y_train[i]\n",
    "    V[:,i] = flat(sqlossgradient(w,[x],[y]))\n",
    "end\n",
    "V /= Ntrain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U matrix\n",
    "U = zeros(Nweights,Nweights,Ntrain)\n",
    "for i=1:Ntrain\n",
    "    x=x_train[i]\n",
    "    y=y_train[i]\n",
    "    for α=1:Nweights\n",
    "        U[:,α,i] = flat(sqlossgradgrad(w,[x],[y],α))\n",
    "    end\n",
    "end\n",
    "U /= Ntrain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X vector\n",
    "X = flat(reggradient(w));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y vector\n",
    "Y = zeros(Nweights,Nweights);\n",
    "for j=1:Nweights\n",
    "    Y[:,j] = flat(reggradgrad(w,j))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbatch = floor(Int,Ntrain/Batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z matrix\n",
    "Z = Nbatch * (Y*V - sum(U[:,i,:]*X[i] for i=1:Nweights));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S matrix\n",
    "S = zeros(Nweights,Ntrain,Ntrain)\n",
    "for j=1:Ntrain\n",
    "    S[:,:,j] = sum(U[:,β,:]*V[β,j] for β=1:Nweights)\n",
    "end\n",
    "S *= Nbatch^2;\n",
    "\n",
    "# SDelta\n",
    "SDelta = copy(S)\n",
    "for i=1:Ntrain\n",
    "    SDelta[:,i,i]=zeros(Nweights)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B matrix\n",
    "B = sum(SDelta[:,j,:] for j=1:Ntrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C matrix\n",
    "C = sum(SDelta[:,:,j] for j=1:Ntrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F matrix\n",
    "F = sum(sum(SDelta[:,i,j]*SDelta[:,i,j]' for i=1:Ntrain) for j=1:Ntrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G matrix\n",
    "G = sum(sum(SDelta[:,i,j]*SDelta[:,j,i]' for i=1:Ntrain) for j=1:Ntrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients a_i\n",
    "M = Ntrain\n",
    "m = Batchsize\n",
    "n = Nbatch\n",
    "\n",
    "a0 = (M-m)*(M+m)/(12m^2)\n",
    "a1 = (n+1)*(M-m)/(12(M-1))\n",
    "a2 = (M-m)*((M+m)*(M-4)+6)/(12(M-2)*(M-1)^2)\n",
    "a3 = (M-m)*(M+m-2)/(4(M-1)^2)\n",
    "a4 = -(0.5*(M-m)/(M-1))^2\n",
    "a5 = -(M-m)*(12M+(M-4)*(M+6)*m)/(12M*(M-2)*(M-1)^2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtheory = 0.5*LearningRate^4*(\n",
    "    a0*Z*Z' +\n",
    "    a1*(Z*(B'-C') + (B-C)*Z') +\n",
    "    a2*(B*B') +\n",
    "    a2*(C*C') +\n",
    "    (a3-2a2)*F +\n",
    "   (a4-2a5)*G +\n",
    "    a5*(B*C' + C*B')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtheory./Dmin_ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An approximation for Diffusion and Covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hessmin_e = hessianmatrix(wmin_e,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the approximation for D\n",
    "coefD = a2*(n*LearningRate)^4/2\n",
    "Dapprx = coefD * (Hessmin_e*V*V'*Hessmin_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapprx./Dmin_ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the FD relation to calculate the covariance matrix for D ∼ HVV'H\n",
    "\n",
    "coefD = a2*(n*LearningRate)^4/2\n",
    "h = eigvals(Hessmin_e);\n",
    "O = eigvecs(Hessmin_e);\n",
    "W = O'*V*V'*O\n",
    "hmat = zeros(Nweights,Nweights)\n",
    "for α=1:Nweights\n",
    "    for β=1:Nweights\n",
    "        hmat[α,β] = (2/LearningRate)*coefD*h[α]*h[β]/(h[α]+h[β])\n",
    "    end\n",
    "end\n",
    "Cov_transformed = hmat.*W\n",
    "Covapprx = O*Cov_transformed*O'\n",
    "Covapprx./Cov_ss ./20 # why the extra factor of 20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
