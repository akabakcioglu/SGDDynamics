{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,Printf,GZip,Knet,Plots,LinearAlgebra,Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING THE NOISY GAUSSIAN (TRAINING DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(4);\n",
    "Range=3.0; # range of the x values for the target Gaussian function\n",
    "Incr = 0.03; # determines the number of samples from which we'll learn\n",
    "Noise_std=0.1; # add noise on the Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data from which we'll learn the Gaussian function\n",
    "# obligatory arguments listed before \";\" while optional arguments come after \";\".\n",
    "function gen_noisy_gaussian(;range=1.0,noise=0.1)\n",
    "    x = collect(-Range:Incr:Range)\n",
    "    y = exp.(-x.^2) + randn(length(x))*noise; # additive gaussian noise\n",
    "    return (x,y)\n",
    "end\n",
    "# output is two vectors x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(range=Range,noise=Noise_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain =length(x_train) # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT THE NETWORK AND THE LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenSize = 2; # number of neurons in the hidden layer\n",
    "Batchsize = 10;\n",
    "RegWeight=0.001; # lambda for L2 regularization\n",
    "InitNorm = 0.5; # initial weight norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deep learning package requires a certain array structure for the weights\n",
    "# but it is easier for the later analysis to dump them all into a single column vector\n",
    "function flat(w) # make a single vector out of all weights\n",
    "    return vcat(w[1],w[2],w[3],w[4])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the weight array from the flat weight vector\n",
    "function unflat(wf)\n",
    "    return [wf[1:HiddenSize],wf[HiddenSize+1:2*HiddenSize],wf[2*HiddenSize+1:3*HiddenSize],wf[end]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer network performing: tanh.(w[hidden,input] * x[input,batchsize] .+ b[hidden,1])\n",
    "# The dot \".\" is for \"broadcasting\": performing the operation pointwise on each input element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this seed to try different initial weigths w/o changing the training data\n",
    "Random.seed!(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights: w = [w1,w2,w3,w4] -> output = w3*tanh.(w1*x .+ w2) .+ w4\n",
    "w = [rand(HiddenSize),rand(HiddenSize),rand(HiddenSize),rand()];\n",
    "w = InitNorm*w/norm(flat(w)); # rescale w so that the norm is InitNorm\n",
    "Nweights = length(flat(w));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions:\n",
    "# x: (input x batchsize) - input\n",
    "# w[1]: (hidden x input) - (input->hidden) weights\n",
    "# w[2]: (hidden x 1) - (input->hidden) biases\n",
    "# w[3]: (hidden x output) - (hidden-> output) weights\n",
    "# w[4]: (output x 1) - (hidden->output) bias\n",
    "\n",
    "# both x and y are ordered in columns per training data point\n",
    "function loss(w,x,y)\n",
    "    guesses =  w[3]'*tanh.(w[1]*x' .+ w[2]) .+ w[4];\n",
    "    return mean(abs2,y'-guesses) + RegWeight*sum(norm(w[i])^2  for i=1:4)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct the gradient-calculating function. grad() is a \"functional\" whose input and output\n",
    "# is a function. Note that grad() requires loss to be a scalar function\n",
    "lossgradient = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient at the initial w\n",
    "# dw has dimensions of w: each weight w_i is replaced with the gradient wrt w_i\n",
    "dw = lossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING FUNCTION THAT PRESENTS THE TRAINING SET IN RANDOM BATCHES (WITH NO REPLACEMENT)\n",
    "# For random batches WITH replacement, move the line \"perm = ..\" inside the for loop\n",
    "\n",
    "function mytrain!(w;lr=0.1)\n",
    "    Nbatch = floor(Int,Ntrain/Batchsize); # few training samples will be left out if Ntrain/Batchsize != integer\n",
    "#    perm = randperm(Ntrain); # a random permutation of [1:Ntrain] - pick batches as chunks from this array\n",
    "    for nb=0:Nbatch-1\n",
    "    perm = randperm(Ntrain); # a random permutation of [1:Ntrain] - pick batches as chunks from this array        # construct batch\n",
    "        x = [x_train[n] for n in perm[nb*Batchsize+1:(nb+1)*Batchsize]]\n",
    "        y = [y_train[n] for n in perm[nb*Batchsize+1:(nb+1)*Batchsize]]\n",
    "        # calculate gradient over the batch\n",
    "        dw = lossgradient(w,x,y);\n",
    "        # update weights\n",
    "        for i=1:length(w)\n",
    "                w[i] -= lr*dw[i]\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 10000; # For a quick training run\n",
    "LearningRate = 0.001; # Ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Collect weights after each epoch in an array (trajectory)\n",
    "@time weights = [ deepcopy(mytrain!(w,lr=LearningRate)) for epoch=1:Nepoch ];  # copy only copies the top layer, does not descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if the training worked. Compare the learned function with the actual gaussian\n",
    "xplot=collect(-Range:0.01:Range) # create an array of x values within the range\n",
    "y_pred = w[3]'*tanh.(w[1]*xplot' .+ w[2]) .+ w[4] # generate the predicted y values\n",
    "# plot the converged function, the initial gaussian and the noisy training samples\n",
    "plot(xplot,[y_pred',exp.(-xplot.^2)]); scatter!(x_train,y_train,leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs epoch\n",
    "SamplingRate=10;\n",
    "x = collect(1:SamplingRate:Nepoch);\n",
    "y = [loss(weights[i],x_train,y_train) for i in x];\n",
    "plot(x,y)\n",
    "#plot(x,y,xaxis=:log10,yaxis=:log10) # can also plot in log-log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(w,xt,yt,Nb)\n",
    "    Nweights = length(flat(w)) # number of weights, that is, dimensions of the diffusion tensor\n",
    "    Nt = length(xt) # number of training examples to be summed over\n",
    "    prefac = (Nt-Nb)/(2*Nb*(Nt-1))\n",
    "    V = zeros(Nweights,Nt) # initialize the diffusion matrix\n",
    "    for i=1:Nt\n",
    "        x=xt[i]\n",
    "        y=yt[i]\n",
    "        V[:,i] = flat(lossgradient(w,[x],[y]))\n",
    "    end\n",
    "    V /= Nt;\n",
    "    dL = flat(lossgradient(w,xt,yt));\n",
    "    \n",
    "    return (LearningRate)^2 * prefac * (Nt*V*V' - dL*dL')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = flat(weights[1])'; for i=2:Nepoch ws = vcat(ws,flat(weights[i])') end;\n",
    "wfinalflat = ws[end,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = diffusiontensor(weights[Nepoch],x_train,y_train,Batchsize)\n",
    "heatmap(D) # check that it is symmetric, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the Hessian,\n",
    "# define a function returning elements of the loss-gradient vector dL/dw_j\n",
    "\n",
    "function lossgradj(w,x,y,j)\n",
    "    return flat(lossgradient(w,x,y))[j]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function calculating a column of Hessian matrix:\n",
    "# Returns d^2(L)/dw_idw_j for all i and given j\n",
    "lossgradgrad = grad(lossgradj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessianmatrix(w,x,y)\n",
    "    Nw = length(flat(w));\n",
    "    Hess = zeros(Nw,Nw);\n",
    "    for j=1:Nw\n",
    "        Hess[:,j] = flat(lossgradgrad(w,x,y,j))\n",
    "    end\n",
    "    return Hess\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Newton's method get the the true minimum of the full loss function\n",
    "wf = wfinalflat;\n",
    "# implement Newton's method to find the true minimum. 4 steps are enough!\n",
    "for n=1:10\n",
    "    Hess = hessianmatrix(unflat(wf),x_train,y_train)\n",
    "    gradwf = flat(lossgradient(unflat(wf),x_train,y_train))\n",
    "    wf = wf - inv(Hess)*gradwf\n",
    "end\n",
    "wminf = wf[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hessmin = hessianmatrix(unflat(wminf),x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusiontensor at the minimum\n",
    "Dmin = diffusiontensor(unflat(wminf),x_train,y_train,Batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculation is from Michael's overleaf notes:\n",
    "# https://www.overleaf.com/2523873322bvvnxpwnskfk\n",
    "function covariancematrix(D,H;lr=LearningRate)\n",
    "    F = eigen(H);\n",
    "    h = F.values\n",
    "    O = F.vectors\n",
    "    Nw = length(h)\n",
    "    ODO = O'*D*O;\n",
    "    Delta = zeros(Nw,Nw);\n",
    "    for i=1:Nw\n",
    "        for j=1:Nw\n",
    "            Delta[i,j] = ODO[i,j]/(h[i]+h[j])\n",
    "        end\n",
    "    end\n",
    "    return (2/lr)*O*Delta*O'\n",
    "end\n",
    "Covmin = covariancematrix(Dmin,Hessmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: must be zero\n",
    "Hessmin*Covmin + Covmin*Hessmin - (2/LearningRate)*Dmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the probability current at a given w\n",
    "# again, from Michael's notes\n",
    "function currentvec(w,x,y; lr=LearningRate)\n",
    "    deltaw = flat(w)-wminf\n",
    "    H = hessianmatrix(w,x,y);\n",
    "    return (Dmin*inv(covariancematrix(Dmin,H)) - lr*H)*deltaw \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the vector fields\n",
    "\n",
    "# scan the vicinity of wstar the get a sense of the current vector field\n",
    "# Since we want a 2d plot, I scan two weights only. Other weights are fixed to\n",
    "# their values at wmin.\n",
    "\n",
    "function gen_grid(wcenter,x_index,y_index,Nx,Ny,Lx,Ly)\n",
    "    warray = wcenter;\n",
    "    for nx=1:Nx\n",
    "        for ny=1:Ny\n",
    "            dw = wcenter-wcenter # zero\n",
    "            dw[x_index] = -Lx + 2*Lx*nx/Nx\n",
    "            dw[y_index] = -Ly + 2*Ly*ny/Ny\n",
    "            warray = hcat(warray,wcenter + dw)\n",
    "        end\n",
    "    end\n",
    "    return warray\n",
    "end\n",
    "\n",
    "x_index = 1;\n",
    "y_index = 7;\n",
    "\n",
    "warray = gen_grid(wminf,x_index,y_index,10,10,0.01,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of (w5,w6) values. wstar is shown in red\n",
    "scatter(warray[x_index,:].-wminf[x_index],warray[y_index,:].-wminf[y_index]); scatter!([0],[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate current vectors\n",
    "currents = currentvec(unflat(warray[:,1]),x_train,y_train)\n",
    "npts = length(warray[1,:])\n",
    "for i=2:npts\n",
    "    currents = hcat(currents,currentvec(unflat(warray[:,i]),x_train,y_train))\n",
    "end\n",
    "currents /= norm(currents[:,2]) # magnitude of current vectors are huge.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current vector field\n",
    "x = warray[x_index,:];\n",
    "y = warray[y_index,:];\n",
    "u_cur = currents[x_index,:]/100;\n",
    "v_cur = currents[y_index,:]/100;\n",
    "quiver(x.-wminf[x_index], y.-wminf[y_index], quiver=(u_cur, v_cur));  scatter!([0],[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
