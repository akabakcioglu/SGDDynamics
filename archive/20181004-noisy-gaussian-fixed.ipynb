{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,LinearAlgebra,Printf,GZip,Knet,Images,Distributions,Plots #dy: do you need all this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=31; # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Range=2.0; # range of the x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseRatio=0.3; # max noise/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gen_noisy_gaussian(len;range=1.0,noiseratio=0.1)\n",
    "    x = sort(randn(len)*range) #dy:randn does the same thing. # sort(rand(Normal(0,1),len)*range)\n",
    "    y = exp.(-x.^2).*(-noiseratio*2*(rand(len).-0.5).+1) # fractional noise\n",
    "    return (x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(L,range=Range,noiseratio=NoiseRatio);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layersize = 3; # number of neurons in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0.1*rand(1,Layersize),0.1*rand(1,Layersize),0.1*rand(1,Layersize),0.1*rand(1,Layersize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dy: did you really mean to use .* here?  neural net layers use matmul. also biases are lower dim.\n",
    "function loss(w,x,y)\n",
    "    guesses = sum(tanh.(w[1].*x.+w[2]).*w[3] .+ w[4],dims=2) # w[1]=w, w[2]=w0, w[3]=w', w[4]=w0'\n",
    "    return mean(abs2,y-guesses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = x_train,y_train\n",
    "sum(tanh.(w[1].*x.+w[2]).*w[3] .+ w[4],dims=2)\n",
    "typeof(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lossgradient = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = lossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show w[1][1]\n",
    "@show dw[1][1]\n",
    "w[1][1] -= 0.1*dw[1][1]\n",
    "@show w[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mytrain!(w;lr=0.1)\n",
    "    dw = lossgradient(w,x_train,y_train);\n",
    "    for i = 1:length(w)\n",
    "        for j = 1:length(w[i])\n",
    "            w[i][j] -= lr*dw[i][j];\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [ deepcopy(mytrain!(w,lr=0.1)) for epoch=1:10 ]  # copy only copies the top layer, does not descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
