{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Statistics, LinearAlgebra, Base.Iterators, Random, StatsBase\n",
    "ARRAY = Array{Float64}\n",
    "LAMBDA=1e-4 # regularization parameter\n",
    "LR=1e-3     # learning rate\n",
    "XSIZE=1   # input dimension\n",
    "YSIZE=1    # output dimension\n",
    "BATCHSIZE=10   # batch size\n",
    "DITER=10000 # iterations for diffusion tensor\n",
    "CITER=10000 # iterations for covariance matrix\n",
    "CINIT=5000  # throw away this many iterations from trajectory for covariance calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Range=3.0; # range of the x values for the target Gaussian function\n",
    "Incr = 0.03; # determines the number of samples from which we'll learn\n",
    "Noise_std=0.1; # add noise on the Gaussian\n",
    "HiddenSize=2;\n",
    "function gen_noisy_gaussian(;range=1.0,noise=0.1)\n",
    "    x = collect(-Range:Incr:Range)\n",
    "    y = exp.(-x.^2) + randn(length(x))*noise; # additive gaussian noise\n",
    "    return (ARRAY(x),ARRAY(y))\n",
    "end\n",
    "Random.seed!(4);\n",
    "(xtrn,ytrn) = gen_noisy_gaussian(range=Range,noise=Noise_std);\n",
    "pop!(xtrn);pop!(ytrn);\n",
    "atrn = xtrn # atrn is used in some boxes below\n",
    "#@show size(xtrn),size(ytrn)\n",
    "\n",
    "function flat(w) # make a single vector out of all weights\n",
    "    return vcat(w[1],w[2],w[3],w[4])\n",
    "end\n",
    "\n",
    "function unflat(wf)\n",
    "    return (wf[1:HiddenSize],wf[HiddenSize+1:2*HiddenSize],wf[2*HiddenSize+1:3*HiddenSize],wf[end])\n",
    "end\n",
    "\n",
    "function pred(wf,x) # returns a row of predicted values for each sample in x\n",
    "    w = unflat(wf)\n",
    "    return w[3]'*tanh.(w[1]*x' .+ w[2]) .+ w[4]\n",
    "end\n",
    "\n",
    "function loss(wf,x,y)\n",
    "    return mean(abs2,y'-pred(wf,x)) + (LAMBDA/2)*sum(abs2,wf) \n",
    "    # wf is 7 dimensional, the following assumes it is the unflat version\n",
    "    # sum(norm(wf[i])^2  for i=1:4)\n",
    "end\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "println.(summary.((xtrn,ytrn)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum without minibatching\n",
    "# ~50 iters/sec, converges to .267218 in 3 mins\n",
    "wminfile = \"wmin_cov_test.jld2\"\n",
    "if true # !isfile(wminfile)\n",
    "#    wmin = Param(ARRAY(zeros(XSIZE*YSIZE)))\n",
    "    wmin = Param(ARRAY(0.1*randn(7))) # cannot init with zero for multi-layer net\n",
    "    args = repeat([(wmin,xtrn,ytrn)],10000)\n",
    "    Knet.gc()\n",
    "    losses = collect(progress(adam(loss,args)))\n",
    "    Knet.save(wminfile, \"wmin\", wmin, \"losses\", losses)\n",
    "else\n",
    "    wmin, losses = Knet.load(wminfile, \"wmin\", \"losses\");\n",
    "end\n",
    "@show value(wmin)\n",
    "losses[end-4:end]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xtrn,[ytrn pred(wmin,xtrn)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian of loss around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessian(loss,w,x,y)\n",
    "    ∇loss = grad(loss)\n",
    "    ∇lossi(w,x,y,i) = ∇loss(w,x,y)[i]\n",
    "    ∇∇lossi = grad(∇lossi)\n",
    "    w = value(w)\n",
    "    n = length(w)\n",
    "    h = similar(w,n,n)\n",
    "    for i in progress(1:n)\n",
    "        h[:,i] .= vec(∇∇lossi(w,x,y,i))\n",
    "    end\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hessian: ~6 mins, ~4:20 with slower _logp? TODO: reoptimize loss.jl\n",
    "hessfile = \"hess_cov_test.jld2\"\n",
    "if true # !isfile(hessfile)\n",
    "    Knet.gc()\n",
    "    hmin = hessian(loss,wmin,atrn,ytrn)\n",
    "    Knet.save(hessfile,\"h\",hmin)\n",
    "else\n",
    "    hmin = Knet.load(hessfile,\"h\")\n",
    "end\n",
    "println.((summary(hmin),extrema(Array(hmin)),norm(hmin),norm(hmin-hmin')));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heigfile = \"heig_cov_test.jld2\"\n",
    "H = Symmetric(Array(0.5*(hmin + hmin')))\n",
    "if true # !isfile(heigfile)\n",
    "    @time eigenH = eigen(H) # ~53s\n",
    "    Knet.save(heigfile,\"eigenH\",eigenH)\n",
    "else\n",
    "    eigenH = Knet.load(heigfile,\"eigenH\")\n",
    "end\n",
    "eigenH.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(eigenH.values, yscale=:log10) |> display\n",
    "#describe(eigenH.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian (numeric check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(w) ≈ f(wmin) + (w-wmin)' g + 1/2 (w-wmin)' H (w-wmin)\n",
    "# Gradient at wmin is ≈0, so the middle term can be assumed 0\n",
    "df = @diff loss(wmin,atrn,ytrn)\n",
    "J = vec(grad(df, wmin)); @show summary(J)\n",
    "@show norm(J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test approx at ~0.1 distance around wmin\n",
    "# adding first order term does not make much difference as expected\n",
    "wrnd = 0.1 * randn!(similar(wmin)) / sqrt(length(wmin))\n",
    "lossw(w) = loss(w,atrn,ytrn)\n",
    "@show lossw(wmin)\n",
    "@show lossw(wmin + wrnd)\n",
    "@show lossw(wmin) + 0.5 * wrnd' * hmin * wrnd\n",
    "@show lossw(wmin) + J' * wrnd + 0.5 * wrnd' * hmin * wrnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatching for SGD-I, i.e. with replacement. Knet.minibatch can't do this, we define new struct\n",
    "struct MB; x; y; n; end\n",
    "Base.Iterators.IteratorSize(::MB) = Base.IsInfinite()\n",
    "#Base.iterate(d::MB, s...)=(r = rand(1:length(d.y),d.n); ((ARRAY(mat(d.x)[:,r]), d.y[r]), true))\n",
    "Base.iterate(d::MB, s...)=(r = rand(1:length(d.y),d.n); ((d.x[r], d.y[r]), true))\n",
    "dtrn = MB(xtrn, ytrn, BATCHSIZE)\n",
    "println.(summary.((xtrn,ytrn,first(dtrn)...)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(loss,w,x,y;iters=DITER,lr=LR,batchsize=BATCHSIZE)\n",
    "    ∇loss = grad(loss)\n",
    "    grad0 = ∇loss(w, ARRAY(x), y)\n",
    "    data = MB(x,y,batchsize)\n",
    "    grads = ( ∇loss(w,x,y) for (x,y) in take(data,iters) )\n",
    "    prefac = lr^2/(2iters)\n",
    "    v = ARRAY(zeros(length(w),length(w)))\n",
    "    for g in progress(grads)\n",
    "        e=vec(grad0-g)\n",
    "        axpy!(prefac,e*e',v)\n",
    "    end\n",
    "    return Array(v)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute diffusion tensor ~20 iters/sec, ~1000 iters/min\n",
    "dtfile = \"dt_cov_test.jld2\"\n",
    "if true # !isfile(dtfile)\n",
    "    Knet.gc()\n",
    "    D = diffusiontensor(loss,wmin,xtrn,ytrn)\n",
    "    Knet.save(dtfile,\"D\",D)\n",
    "else\n",
    "    D = Knet.load(dtfile,\"D\")\n",
    "end\n",
    "summarystats(vec(D)) |> dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convergence (with LR=0.1):\n",
    "#norm(d100),norm(d1000),norm(d100-d1000)\n",
    "#(9.006901905269366e-5, 7.966566524654371e-5, 4.746245825247884e-5)\n",
    "#norm(d1000),norm(d2000),norm(d1000-d2000)\n",
    "#(7.966566524654371e-5, 7.976183314696048e-5, 1.8908596356951573e-5)\n",
    "#norm(d4000),norm(d2000),norm(d4000-d2000)\n",
    "#(8.024754500933944e-5, 7.976183314696048e-5, 1.3446098748867312e-5)\n",
    "#norm(d10000),norm(d4000),norm(d10000-d4000)\n",
    "#(7.942869188760434e-5, 8.024754500933944e-5, 8.963487531775732e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deigfile = \"deig_cov_test.jld2\"\n",
    "if true # !isfile(deigfile)\n",
    "    @time eigenD = eigen(Symmetric(D)) # ~53s\n",
    "    Knet.save(deigfile,\"eigenD\",eigenD)\n",
    "else\n",
    "    eigenD = Knet.load(deigfile,\"eigenD\")\n",
    "end\n",
    "eigenD.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(eigenD.values .+ 1e-23, yscale=:log10) |> display\n",
    "#summarystats(eigenD.values) |> dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record trajectory with SGD starting at minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory of w starting from wmin recorded after each update: \n",
    "# ~1000 updates/sec, ~16 secs total\n",
    "trajfile = \"traj_cov_test.jld2\"\n",
    "if true # !isfile(trajfile)\n",
    "    w = Param(ARRAY(value(wmin)))\n",
    "    data = MB(xtrn,ytrn,BATCHSIZE)\n",
    "    d = take(data,CITER-1)\n",
    "    W = zeros(eltype(w),length(w),1+length(d))\n",
    "    i = 1; W[:,i] = Array(vec(w))\n",
    "    f(x,y) = loss(w,x,y)\n",
    "    Knet.gc()\n",
    "    for t in progress(sgd(f,d; lr=LR))\n",
    "        i += 1\n",
    "        W[:,i] = Array(vec(w))\n",
    "    end\n",
    "    Knet.save(trajfile,\"W\",W)\n",
    "else\n",
    "    W = Knet.load(trajfile,\"W\")\n",
    "end\n",
    "summary(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot losses on whole dataset, first steps seem transient, ~10 secs\n",
    "r = 1:10:size(W,2)\n",
    "@time plot(r, [loss(ARRAY(W[:,i]),atrn,ytrn) for i in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory of two random dimensions\n",
    "# Seems to converge to a slightly different point?\n",
    "# Interesting patterns: staircase, globe, H shaped\n",
    "@show r1,r2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "scatter(W[r1,1:end],W[r2,1:end])\n",
    "scatter!(W[r1,1:10], W[r2,1:10],mc=:red) # mark beginning with red\n",
    "scatter!(W[r1,end-9:end],W[r2,end-9:end],mc=:yellow) # mark end with yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training seems to converge to a slightly worse spot\n",
    "w0 = Array(value(wmin))\n",
    "μ = mean(W[:,CINIT:end],dims=2)\n",
    "w1 = W[:,end]\n",
    "@show norm(w0), norm(μ), norm(w0 - μ)\n",
    "@show extrema(w0), extrema(μ), extrema(w0 - μ)\n",
    "@show mean(abs.(w0 - μ) .> 0.01)\n",
    "@show loss(w0,xtrn,ytrn)\n",
    "@show loss(μ,xtrn,ytrn)\n",
    "@show loss(w1,xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance of SGD trajectory around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wstable = W[:,CINIT:end];  @show summary(Wstable)\n",
    "μ = mean(Wstable,dims=2); @show summary(μ)\n",
    "Wzero = Wstable .- μ;     @show summary(Wzero)\n",
    "Σ = (Wzero * Wzero') / size(Wzero,2); @show summary(Σ)\n",
    "@show norm(Σ),extrema(Σ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceigfile = \"ceig07.jld2\"\n",
    "if true # !isfile(ceigfile)\n",
    "    @time eigenC = eigen(Symmetric(Σ)) # ~53s\n",
    "    Knet.save(ceigfile,\"eigenC\",eigenC)\n",
    "else\n",
    "    eigenC = Knet.load(ceigfile,\"eigenC\")\n",
    "end\n",
    "eigenC.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(eigenC.values .+ 1e-19, yscale=:log10) |> display\n",
    "#summarystats(eigenC.values) |> dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.((H,D,Σ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = H*Σ + Σ*H\n",
    "b = (2/LR)*D\n",
    "a ≈ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(a),norm(b),norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try fit_mle for covariance: gives the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Σ is not positive definite, MLE fails in mnist because Hessian=inverse(Σ)\n",
    "# Note that this is not the same as the loss Hessian, it is the distribution Hessian!\n",
    "using Distributions\n",
    "fit = fit_mle(MvNormal, Wstable)\n",
    "C = fit.Σ + zeros(7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = H*C + C*H\n",
    "b = (2/LR)*D\n",
    "a ≈ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(a),norm(b),norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C ≈ Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
