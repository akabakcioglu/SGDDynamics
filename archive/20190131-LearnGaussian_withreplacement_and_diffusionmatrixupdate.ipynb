{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,Printf,GZip,Knet,Plots,LinearAlgebra,Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING THE NOISY GAUSSIAN (TRAINING DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(4);\n",
    "Range=3.0; # range of the x values for the target Gaussian function\n",
    "Incr = 0.03; # determines the number of samples from which we'll learn\n",
    "Noise_std=0.1; # add noise on the Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data from which we'll learn the Gaussian function\n",
    "# obligatory arguments listed before \";\" while optional arguments come after \";\".\n",
    "function gen_noisy_gaussian(;range=1.0,noise=0.1)\n",
    "    x = collect(-Range:Incr:Range)\n",
    "    y = exp.(-x.^2) + randn(length(x))*noise; # additive gaussian noise\n",
    "    return (x,y)\n",
    "end\n",
    "# output is two vectors x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(range=Range,noise=Noise_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain =length(x_train) # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT THE NETWORK AND THE LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenSize = 2; # number of neurons in the hidden layer\n",
    "Batchsize = 10;\n",
    "RegWeight=0.001; # lambda for L2 regularization\n",
    "InitNorm = 0.5; # initial weight norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deep learning package requires a certain array structure for the weights\n",
    "# but it is easier for the later analysis to dump them all into a single column vector\n",
    "function flat(w) # make a single vector out of all weights\n",
    "    return vcat(w[1],w[2],w[3],w[4])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the weight array from the flat weight vector\n",
    "function unflat(wf)\n",
    "    return [wf[1:HiddenSize],wf[HiddenSize+1:2*HiddenSize],wf[2*HiddenSize+1:3*HiddenSize],wf[end]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer network performing: tanh.(w[hidden,input] * x[input,batchsize] .+ b[hidden,1])\n",
    "# The dot \".\" is for \"broadcasting\": performing the operation pointwise on each input element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this seed to try different initial weigths w/o changing the training data\n",
    "Random.seed!(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights: w = [w1,w2,w3,w4] -> output = w3*tanh.(w1*x .+ w2) .+ w4\n",
    "w = [rand(HiddenSize),rand(HiddenSize),rand(HiddenSize),rand()];\n",
    "w = InitNorm*w/norm(flat(w)); # rescale w so that the norm is InitNorm\n",
    "Nweights = length(flat(w));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions:\n",
    "# x: (input x batchsize) - input\n",
    "# w[1]: (hidden x input) - (input->hidden) weights\n",
    "# w[2]: (hidden x 1) - (input->hidden) biases\n",
    "# w[3]: (hidden x output) - (hidden-> output) weights\n",
    "# w[4]: (output x 1) - (hidden->output) bias\n",
    "\n",
    "# both x and y are ordered in columns per training data point\n",
    "function loss(w,x,y)\n",
    "    guesses =  w[3]'*tanh.(w[1]*x' .+ w[2]) .+ w[4];\n",
    "    return mean(abs2,y'-guesses) + RegWeight*sum(norm(w[i])^2  for i=1:4)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct the gradient-calculating function. grad() is a \"functional\" whose input and output\n",
    "# is a function. Note that grad() requires loss to be a scalar function\n",
    "lossgradient = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient at the initial w\n",
    "# dw has dimensions of w: each weight w_i is replaced with the gradient wrt w_i\n",
    "dw = lossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function calculating the diffusion tensor at the point w for a given batchsize\n",
    "# The prefactor (learning_rate)^2 is NOT INCLUDED\n",
    "\n",
    "function diffusiontensor(w,xt,yt,Nb) # xt=x_train, yt=y_train, Nb=Batchsize\n",
    "Nweights = length(flat(w)) # number of weights, that is, dimensions of the diffusion tensor\n",
    "Nt = length(xt) # number of training examples to be summed over\n",
    "D = zeros(Nweights,Nweights) # initialize the diffusion matrix\n",
    "Prefac = ((1/Nb)-(1/Nt))/(2*Nt)\n",
    "for i=1:Nt\n",
    "    x=xt[i]\n",
    "    y=yt[i]\n",
    "    dwflat = flat(lossgradient(w,[x],[y]))\n",
    "    D += dwflat.*dwflat'\n",
    "end\n",
    "D *= Prefac\n",
    "return D\n",
    "end\n",
    "\n",
    "# use the diffusion tensor at the loss minimum\n",
    "wmin = [-1.1100157770342636,1.2336447839089348,0.6175562778179838,0.6682464780489719,0.9071070290803469,0.8871744628416607,-0.0297615338498045]\n",
    "Dmin = diffusiontensor(unflat(wmin),x_train,y_train,Batchsize)\n",
    "Dmin = Dmin/norm(Dmin,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING FUNCTION THAT PRESENTS THE TRAINING SET IN RANDOM BATCHES (WITH NO REPLACEMENT)\n",
    "# For random batches WITH replacement, move the line \"perm = ..\" inside the for loop\n",
    "\n",
    "function mytrain!(w;lr=0.1)\n",
    "    Nbatch = floor(Int,Ntrain/Batchsize); # few training samples will be left out if Ntrain/Batchsize != integer\n",
    "    for nb=0:Nbatch-1\n",
    "        # construct batch\n",
    "        perm = randperm(Ntrain); # a random permutation of [1:Ntrain] - pick batches as chunks from this array\n",
    "        x = [x_train[n] for n in perm[nb*Batchsize+1:(nb+1)*Batchsize]]\n",
    "        y = [y_train[n] for n in perm[nb*Batchsize+1:(nb+1)*Batchsize]]\n",
    "        # calculate gradient over the batch\n",
    "#        dw = lossgradient(w,x,y);\n",
    "        dw = unflat(Dmin*flat(lossgradient(w,x,y)));\n",
    "        # update weights\n",
    "        for i=1:length(w)\n",
    "                w[i] -= lr*dw[i]\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 10000; # For a quick training run\n",
    "LearningRate = 0.5; # Ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Collect weights after each epoch in an array (trajectory)\n",
    "@time weights = [ deepcopy(mytrain!(w,lr=LearningRate)) for epoch=1:Nepoch ];  # copy only copies the top layer, does not descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if the training worked. Compare the learned function with the actual gaussian\n",
    "xplot=collect(-Range:0.01:Range) # create an array of x values within the range\n",
    "y_pred = w[3]'*tanh.(w[1]*xplot' .+ w[2]) .+ w[4] # generate the predicted y values\n",
    "# plot the converged function, the initial gaussian and the noisy training samples\n",
    "plot(xplot,[y_pred',exp.(-xplot.^2)]); scatter!(x_train,y_train,leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs epoch\n",
    "SamplingRate=10;\n",
    "x = collect(1:SamplingRate:Nepoch);\n",
    "y = [loss(weights[i],x_train,y_train) for i in x];\n",
    "#plot(x,y)\n",
    "plot(x,y,xaxis=:log10,yaxis=:log10) # can also plot in log-log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
