{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,LinearAlgebra,Printf,GZip,Knet,Images,Distributions,Plots #dy: do you need all this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=31; # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Range=2.0; # range of the x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseRatio=0.3; # max noise/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gen_noisy_gaussian(len;range=1.0,noiseratio=0.1)\n",
    "    x = sort(randn(len)*range) #dy:randn does the same thing. # sort(rand(Normal(0,1),len)*range)\n",
    "    y = exp.(-x.^2).*(-noiseratio*2*(rand(len).-0.5).+1) # fractional noise\n",
    "    return (x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(L,range=Range,noiseratio=NoiseRatio);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = permutedims(x_train)\n",
    "y_train = permutedims(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layersize = 3; # number of neurons in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = 1\n",
    "# input = 1\n",
    "# hidden = 3\n",
    "# batchsize = 31\n",
    "#relu.(w[hidden,input] * x[input,batchsize] .+ b[hidden,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0.1*rand(Layersize,1),0.1*rand(Layersize,1),0.1*rand(1,Layersize),0.1*rand(1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = sum(w[3] * tanh.(w[1]*x.+w[2]) .+ w[4],dims=1) # w[1]=w, w[2]=w0, w[3]=w', w[4]=w0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w[1]: (1,3)\n",
    "#x: (31,1)\n",
    "#dy: did you really mean to use .* here?  neural net layers use matmul. also biases are lower dim.\n",
    "function loss(w,x,y)\n",
    "    guesses = sum(w[3] * tanh.(w[1]*x.+w[2]) .+ w[4],dims=1) # w[1]=w, w[2]=w0, w[3]=w', w[4]=w0'\n",
    "    return mean(abs2,y-guesses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(w,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lossgradient = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = lossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mytrain!(w;lr=0.1)\n",
    "    dw = lossgradient(w,x_train,y_train);\n",
    "    for i = 1:length(w)\n",
    "        for j = 1:length(w[i])\n",
    "            w[i][j] -= lr*dw[i][j];\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [ deepcopy(mytrain!(w,lr=0.1)) for epoch=1:10 ]  # copy only copies the top layer, does not descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w[hidden, input] * x[input, batchsize] => z[hidden,batchsize] .+ b[hidden,1] => y[hidden,batchsize]\n",
    "\n",
    "x1 = input\n",
    "#x1 = dropout(x1,0.5)\n",
    "x2 = tanh.(w1 * x1 .+ b1)\n",
    "x2 = dropout(x2,0.5)\n",
    "x3 = tanh.(w2 * x2 .+ b2)\n",
    "#x3 = dropout(x3,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
