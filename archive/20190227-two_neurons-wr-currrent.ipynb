{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,Printf,GZip,Knet,Plots,LinearAlgebra,Distributions #,Interact,StatsBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples from a noisy Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV[\"GRDIR\"]=\"\"\n",
    "# Pkg.build(\"GR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(4);\n",
    "Range=3.0; # range of the x values for the target Gaussian function\n",
    "Incr = 0.03; # determines the number of samples from which we'll learn\n",
    "Noise_std=0.1; # add noise on the Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data from which we'll learn the Gaussian function\n",
    "# obligatory arguments listed before \";\" while optional arguments come after \";\".\n",
    "function gen_noisy_gaussian(;range=1.0,noise=0.1)\n",
    "    x = collect(-Range:Incr:Range)\n",
    "    y = exp.(-x.^2) + randn(length(x))*noise; # additive gaussian noise\n",
    "    return (x,y)\n",
    "end\n",
    "# output is two vectors x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(range=Range,noise=Noise_std);\n",
    "pop!(x_train);pop!(y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain =length(x_train) # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenSize = 2; # number of neurons in the hidden layer\n",
    "Batchsize = 10;\n",
    "RegWeight=0.001; # lambda for L2 regularization\n",
    "InitNorm = 0.5; # initial weight norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deep learning package requires a certain array structure for the weights\n",
    "# but it is easier for the later analysis to dump them all into a single column vector\n",
    "function flat(w) # make a single vector out of all weights\n",
    "    return vcat(w[1],w[2],w[3],w[4])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the weight array from the flat weight vector\n",
    "function unflat(wf)\n",
    "    return [wf[1:HiddenSize],wf[HiddenSize+1:2*HiddenSize],wf[2*HiddenSize+1:3*HiddenSize],wf[end]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this seed to try different initial weigths w/o changing the training data\n",
    "Random.seed!(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights: w = [w1,w2,w3,w4] -> output = w3*tanh.(w1*x .+ w2) .+ w4\n",
    "w = [rand(HiddenSize),rand(HiddenSize),rand(HiddenSize),rand()];\n",
    "w = InitNorm*w/norm(flat(w)); # rescale w so that the norm is InitNorm\n",
    "Nweights = length(flat(w));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(w,x) # returns a row of predicted values for each sample in x\n",
    "    return w[3]'*tanh.(w[1]*x' .+ w[2]) .+ w[4]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both x and y are ordered in columns per training data point\n",
    "function sqloss(w,x,y)\n",
    "    return mean(abs2,y'-predict(w,x))\n",
    "end\n",
    "\n",
    "function reg(w)\n",
    "    return RegWeight*sum(norm(w[i])^2  for i=1:4)\n",
    "end\n",
    "\n",
    "function loss(w,x,y)\n",
    "    return sqloss(w,x,y) + reg(w)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-calculating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grad() is a \"functional\" whose input and output\n",
    "# is a function. Note that grad() requires loss to be a scalar function\n",
    "lossgradient = grad(loss)\n",
    "sqlossgradient = grad(sqloss)\n",
    "reggradient = grad(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient at the initial w\n",
    "# dw has dimensions of w: each weight w_i is replaced with the gradient wrt w_i\n",
    "dw = lossgradient(w,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function (with replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchtrain!(w,lr)\n",
    "perm = randperm(Ntrain)[1:Batchsize]; # a random permutation of [1:Ntrain] - pick batches as chunks from this array        # construct batch\n",
    "x = [x_train[n] for n in perm]\n",
    "y = [y_train[n] for n in perm]\n",
    "# calculate gradient over the batch\n",
    "dw = lossgradient(w,x,y);\n",
    "# update weights\n",
    "for i=1:length(w)\n",
    "    w[i] -= lr*dw[i]\n",
    "end\n",
    "return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING FUNCTION THAT PRESENTS THE TRAINING SET IN RANDOM BATCHES (WITH NO REPLACEMENT)\n",
    "# For random batches WITH replacement, move the line \"perm = ..\" inside the for loop\n",
    "\n",
    "function mytrain!(w;lr=0.1)\n",
    "    Nbatch = floor(Int,Ntrain/Batchsize); # few training samples will be left out if Ntrain/Batchsize != integer\n",
    "    for nb=1:Nbatch\n",
    "        batchtrain!(w,lr)\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 2000; # For a quick training run\n",
    "η = 0.001; # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Increase learning rate by a factor of 10 for the initial run\n",
    "@time w_training = [ deepcopy(mytrain!(w,lr=10*η)) for i=1:Nepoch ];  # copy only copies the top layer, does not descend.\n",
    "wf_training = zeros(Nepoch,Nweights); for i=1:Nepoch wf_training[i,:] = flat(w_training[i]) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if the training worked. Compare the learned function with the actual gaussian\n",
    "xplot=collect(-Range:0.01:Range)\n",
    "# plot the converged function, the initial gaussian and the noisy training samples\n",
    "plot(xplot,[predict(w,xplot)',exp.(-xplot.^2)]); scatter!(x_train,y_train,leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs epoch\n",
    "SamplingRate=10;\n",
    "x = collect(1:SamplingRate:Nepoch);\n",
    "y = [loss(w_training[i],x_train,y_train) for i in x];\n",
    "plot(x,y)\n",
    "#plot(x,y,xaxis=:log10,yaxis=:log10) # can also plot in log-log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion tensor, Hessian, Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(w,xt,yt,Nb,lr)\n",
    "    Nweights = length(flat(w)) # number of weights, that is, dimensions of the diffusion tensor\n",
    "    Nt = length(xt) # number of training examples to be summed over\n",
    "    prefac = (Nt-Nb)/(2*Nb*(Nt-1))\n",
    "    V = zeros(Nweights,Nt) # initialize the diffusion matrix\n",
    "    for i=1:Nt\n",
    "        x=xt[i]\n",
    "        y=yt[i]\n",
    "        V[:,i] = flat(lossgradient(w,[x],[y]))\n",
    "    end\n",
    "    V /= Nt;\n",
    "    dL = flat(lossgradient(w,xt,yt));\n",
    "    \n",
    "    return lr^2 * prefac * (Nt*V*V' - dL*dL')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the diffusion tensor by sampling the noise\n",
    "function diffusiontensor_num(w,n,lr) # n: number of samples used for estimation\n",
    "    wstart = deepcopy(w)\n",
    "    wlist = zeros(Nweights,n)\n",
    "    \n",
    "    for i=1:n\n",
    "        ww = deepcopy(wstart)\n",
    "        batchtrain!(ww,lr)\n",
    "        wlist[:,i] = flat(ww)\n",
    "    end\n",
    "\n",
    "    # subtract mean\n",
    "    wlist .-= sum(wlist[:,i] for i=1:n)/n\n",
    "\n",
    "    D = zeros(Nweights,Nweights)\n",
    "    for α=1:Nweights\n",
    "        for β=1:Nweights\n",
    "            for i=1:n\n",
    "                D[α,β] += wlist[α,i]*wlist[β,i]\n",
    "            end\n",
    "            D[α,β] /= n;\n",
    "        end\n",
    "    end\n",
    "    return 0.5*D\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the Hessian,\n",
    "# define a function returning elements of the loss-gradient vector dL/dw_j\n",
    "\n",
    "function lossgradj(w,x,y,j)\n",
    "    return flat(lossgradient(w,x,y))[j]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function calculating a column of Hessian matrix:\n",
    "# Returns d^2(L)/dw_idw_j for all i and given j\n",
    "lossgradgrad = grad(lossgradj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessianmatrix(w,x,y)\n",
    "    Nw = length(flat(w));\n",
    "    Hess = zeros(Nw,Nw);\n",
    "    for j=1:Nw\n",
    "        Hess[:,j] = flat(lossgradgrad(w,x,y,j))\n",
    "    end\n",
    "    return Hess\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculation is from Michael's overleaf notes:\n",
    "# https://www.overleaf.com/2523873322bvvnxpwnskfk\n",
    "function covariancematrix(D,H,lr) # is a function of the learning rate\n",
    "    F = eigen(H);\n",
    "    h = F.values\n",
    "    O = F.vectors\n",
    "    Nw = length(h)\n",
    "    ODO = O'*D*O;\n",
    "    Delta = zeros(Nw,Nw);\n",
    "    for i=1:Nw\n",
    "        for j=1:Nw\n",
    "            Delta[i,j] = ODO[i,j]/(h[i]+h[j])\n",
    "        end\n",
    "    end\n",
    "    return (2/lr)*O*Delta*O'\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Newton's Method to find the minimum of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Newton's method get the the true minimum of the full loss function\n",
    "wf = flat(w_training[end]);\n",
    "# implement Newton's method to find the true minimum. 4 steps are enough!\n",
    "for n=1:10\n",
    "    Hess = hessianmatrix(unflat(wf),x_train,y_train)\n",
    "    gradwf = flat(lossgradient(unflat(wf),x_train,y_train))\n",
    "    wf = wf - inv(Hess)*gradwf\n",
    "end\n",
    "\n",
    "wminf = wf[:,1]\n",
    "wmin = unflat(wminf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossgradient(wmin,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian at the loss minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hessmin = hessianmatrix(wmin,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals(Hessmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion tensor at the loss minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion tensor at the loss minimum (using VV')\n",
    "Dmin = diffusiontensor(wmin,x_train,y_train,Batchsize,η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with D calculated numerically (using 10000 trajectory points)\n",
    "diffusiontensor_num(wmin,10000,η) ./ Dmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix as a function of Hessian and Diffusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Covmin = covariancematrix(Dmin,Hessmin,η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the math is right: HC+CH = (2/η)D\n",
    "Hessmin*Covmin + Covmin*Hessmin - (2/η)*Dmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1) # Verified that the results don't change for different seeds.\n",
    "Nmarkov = 3000000; # number of memoryless \"Markovian\" steps\n",
    "\n",
    "trans = 500000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = deepcopy(wmin); # start from the minimum of the potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time w_ss = [ deepcopy(batchtrain!(w,η)) for step=1:Nmarkov ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the flat trajectory\n",
    "@time wf_ss = zeros(Nmarkov,Nweights); for i=1:Nmarkov wf_ss[i,:] = flat(w_ss[i]) end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the steady-state distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight indices to visualize\n",
    "xid = 5\n",
    "yid = 6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "\n",
    "using StatsBase\n",
    "\n",
    "ss_range=collect(trans:Nmarkov)\n",
    "wx = wf_ss[ss_range,xid]\n",
    "wy = wf_ss[ss_range,yid]\n",
    "resxy=(200,200) # histogram bins\n",
    "\n",
    "fith = fit(Histogram,(wx,wy),nbins=resxy)\n",
    "\n",
    "fith.weights # bin counts\n",
    "fith.edges # bin boundaries\n",
    "maxhist=maximum(fith.weights) # will use later for better looking plots\n",
    "\n",
    "histogram2d(wx,wy,bins=resxy)\n",
    "scatter!([wminf[xid,1]],[wminf[yid,1]],leg=false,markercolor=\"cyan\",markersize=4) # loss minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Mv-Gaussian to the equilibrium data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fit_ss = fit_mle(MvNormal,wf_ss[ss_range,:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steady-state mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanwf = Distributions.mean(Fit_ss)\n",
    "meanw = unflat(meanwf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix (from ss-trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_ss = Distributions.cov(Fit_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with the solution of ΣH + HΣ = (2/η)D\n",
    "Cov_ss./covariancematrix(Dmin,Hessmin,η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x2 submatrix that goes into the exponent of the projected fit\n",
    "Cov_xy_inv = inv(Cov_ss[[xid,yid],[xid,yid]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the steady-state distribution on top of the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a grid enclosing the steady-state trajectory\n",
    "minmaxdiff(t) = maximum(t)-minimum(t)\n",
    "\n",
    "function makegrid(xvec,yvec,mean,xindex,yindex;Nx=10,Ny=10,zoom=0.75)\n",
    "    Lx,Ly = minmaxdiff(xvec),minmaxdiff(yvec)\n",
    "    xrange = zoom*Lx\n",
    "    yrange = zoom*Ly\n",
    "    dx = xrange/Nx\n",
    "    dy = yrange/Ny\n",
    "    x = collect(-xrange:dx:xrange) .+ mean[xindex]\n",
    "    y = collect(-yrange:dy:yrange) .+ mean[yindex]\n",
    "\n",
    "    # some mumbo-jumbo for calculating weights corresponding to grid points\n",
    "    Identity = Diagonal(ones(Nweights,Nweights)); # unit matrix\n",
    "    xmask = Identity[:,xindex];\n",
    "    ymask = Identity[:,yindex];\n",
    "    Imask = Identity - xmask*xmask' - ymask*ymask' # set two diagonal elements to zero\n",
    "    return (x,y,Imask,xmask,ymask)\n",
    "end\n",
    "\n",
    "(x,y,Imask,xmask,ymask) = makegrid(wx,wy,meanwf,xid,yid)\n",
    "\n",
    "histogram2d(wx,wy,bins=200,aspect_ratio=1.0)\n",
    "\n",
    "meanxy = meanwf[[xid yid]]\n",
    "## mv-Gaussian fit contours\n",
    "fexp(s,t) = -(([s t]-meanxy)*Cov_xy_inv*([s t]-meanxy)')[1]\n",
    "ffit(s,t) =  maxhist * fexp(s,t)/fexp(x[end],y[end])\n",
    "contour!(x,y,ffit,linestyle=:dash)\n",
    "\n",
    "## Loss contours\n",
    "midx = Int((length(x)-1)/2)\n",
    "midy = Int((length(y)-1)/2)\n",
    "fexp(s,t) = loss(unflat(Imask*meanwf + s*xmask + t*ymask),x_train,y_train) - loss(wmin,x_train,y_train)\n",
    "flossxy(s,t) = maxhist * log(fexp(s,t))/log(fexp(x[midx],y[midy]))\n",
    "contour!(x,y,flossxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move to the eigen-coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick two eigen directions\n",
    "Xid = Nweights\n",
    "Yid = Nweights-1\n",
    "\n",
    "O = eigvecs(Cov_ss);\n",
    "\n",
    "# revert to original weights\n",
    "#O *= O' # identity\n",
    "#Xidx = 5\n",
    "#Yidx = 6\n",
    "\n",
    "\n",
    "W_ss = wf_ss*O; # sample weights are row vectors\n",
    "Wx = W_ss[trans:end,Xid]\n",
    "Wy = W_ss[trans:end,Yid]\n",
    "\n",
    "COV_ss = O'*Cov_ss*O\n",
    "COV_xy_inv = inv(COV_ss[[Xid,Yid],[Xid,Yid]])\n",
    "\n",
    "meanW = O'*meanwf\n",
    "Wminf = O'*wminf;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,Imask,xmask,ymask) = makegrid(Wx,Wy,meanW,Xid,Yid)\n",
    "\n",
    "histogram2d(Wx,Wy,bins=200,aspect_ratio=1)\n",
    "\n",
    "meanXY = meanW[[Xid Yid]]\n",
    "# Contours of the fit mv-Gaussian\n",
    "fexp(s,t) = -(([s t]-meanXY)*COV_xy_inv*([s t]-meanXY)')[1]\n",
    "Ffit(s,t) = maxhist* fexp(s,t)/fexp(x[end],y[end])\n",
    "contour!(x,y,Ffit,linestyle=:dash)\n",
    "\n",
    "# contours of loss\n",
    "midx = Int((length(x)-1)/2)\n",
    "midy = Int((length(y)-1)/2)\n",
    "fexp(s,t) = loss(unflat(O*(Imask*meanW + s*xmask + t*ymask)),x_train,y_train) - loss(wmin,x_train,y_train)\n",
    "Flossxy(s,t) = (maxhist/5) * (log(fexp(s,t)) - log(fexp(x[midx],y[midy])))\n",
    "contour!(x,y,Flossxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area-sweep estimation of rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function arealvelocity(traj,x,y,center)\n",
    "    N = length(traj[:,1])\n",
    "    Cosθs=zeros(N-1);\n",
    "    Sinθs=zeros(N-1);\n",
    "    Area=zeros(N-1);\n",
    "    Areasum=zeros(N);\n",
    "    for n=1:N-1\n",
    "        # construct vectors connecting the trajectory points to the center (mean)\n",
    "        v1=[traj[n,x]-center[x],traj[n,y]-center[y],0]\n",
    "        v2=[traj[n+1,x]-center[x],traj[n+1,y]-center[y],0]\n",
    "        v1norm = norm(v1)\n",
    "        v2norm = norm(v2)\n",
    "        # get the angle between them. Sign of Sinθ gives the direction\n",
    "        Cosθs[n] = dot(v1,v2)/(v1norm*v2norm)\n",
    "        Sinθs[n] = cross(v1,v2)[3]/(v1norm*v2norm)\n",
    "        Area[n] = v1norm*v2norm*Sinθs[n]/2\n",
    "        Areasum[n+1] = Areasum[n]+Area[n]\n",
    "    end\n",
    "    \n",
    "    for n=1:N\n",
    "        Areasum[n] /= n\n",
    "    end\n",
    "    \n",
    "    return Areasum   \n",
    "end\n",
    "\n",
    "# test..\n",
    "# N=10000;\n",
    "# circ = zeros(N,2)\n",
    "# for n=1:N\n",
    "#     circ[n,:] = [cos(2π*n/N)+0.1*rand(),sin(2π*n/N)+0.1*rand()]\n",
    "# end\n",
    "# plot(arealvelocity(circ,1,2,[0,0])/(π/N),ylim=[0,2])\n",
    "\n",
    "parr = arealvelocity(W_ss[trans:end,:],Xid,Yid,meanW);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynorm = sqrt(π^2*COV_ss[Xid,Xid]*COV_ss[Yid,Yid]/4)\n",
    "plot(parr[1:100:end]/mynorm,leg=false,ylim=[-0.0001,0.0001]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the probability current at a given w\n",
    "# again, from Michael's notes.\n",
    "\n",
    "# Note that, current is calculated in the original weight basis\n",
    "\n",
    "function currentvec(w,wcenter,x,y,lr)\n",
    "    deltaw = flat(w-wcenter)\n",
    "    H = hessianmatrix(w,x,y);\n",
    "    return (Dmin*inv(covariancematrix(Dmin,H,lr)) - lr*H)*deltaw \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the vector fields\n",
    "\n",
    "# scan the vicinity of ss-distribution's mean (center point of the grid) the \n",
    "# get a sense of the current vector field. Two components are scanned while\n",
    "# the rest are fixed to their value at the mean.\n",
    "\n",
    "function gridarray(wcenter,x_index,y_index,Nx,Ny,Lx,Ly)\n",
    "    warray = zeros(Nweights,(Nx+1)*(Ny+1))\n",
    "    for nx=0:Nx\n",
    "        for ny=0:Ny\n",
    "            dw = wcenter-wcenter # zero\n",
    "            dw[x_index] = -Lx + 2*Lx*nx/Nx\n",
    "            dw[y_index] = -Ly + 2*Ly*ny/Ny\n",
    "            warray[:,1+nx*(Ny+1)+ny] = wcenter + dw\n",
    "        end\n",
    "    end\n",
    "    return warray\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lx = minmaxdiff(Wx)/2\n",
    "Ly = minmaxdiff(Wy)/2\n",
    "Nx = Ny = 10\n",
    "Warray = gridarray(Wminf,Xid,Yid,Nx,Ny,Lx,Ly)\n",
    "\n",
    "# Scatter plot of weight values. wstar is shown in red\n",
    "#scatter(warray[x_index,:],warray[y_index,:]); scatter!([wmin_ef[x_index]],[wmin_ef[y_index]],leg=false)\n",
    "scatter(Warray[Xid,:],Warray[Yid,:]); scatter!([Wminf[Xid]],[Wminf[Yid]],leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate current vectors\n",
    "# Need to back transform the warray in eigen-basis to the original weight basis\n",
    "# using (O*warray), since currentvec() is defined for the original weights\n",
    "\n",
    "npts = length(Warray[1,:])\n",
    "currents = zeros(Nweights,npts)\n",
    "for i=1:npts\n",
    "    currents[:,i] = O'*currentvec(unflat(O*Warray[:,i]),meanw,x_train,y_train,η)\n",
    "end\n",
    "currents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Interact\n",
    "\n",
    "m = @manipulate for xind in slider(1:7,value=5), yind in slider(1:7,value=7)\n",
    "    Wxx = W_ss[trans:end,xind]\n",
    "    Wyy = W_ss[trans:end,yind]\n",
    "    Lx = minmaxdiff(Wxx)/2\n",
    "    Ly = minmaxdiff(Wyy)/2\n",
    "    Nx = Ny = 10\n",
    "    Warr = gridarray(Wminf,xind,yind,Nx,Ny,Lx,Ly)\n",
    "    npts = length(Warr[1,:])\n",
    "    currents = zeros(Nweights,npts)\n",
    "    for i=1:npts\n",
    "        currents[:,i] = O'*currentvec(unflat(O*Warr[:,i]),meanw,x_train,y_train,η)\n",
    "    end\n",
    "    x = Warr[xind,:];\n",
    "    y = Warr[yind,:];\n",
    "    mynorm = 3*sqrt(4*Lx*Ly/(Nx*Ny))/maximum(abs.(currents[[xind yind],:]))\n",
    "    u_cur = mynorm*currents[xind,:];\n",
    "    v_cur = mynorm*currents[yind,:];\n",
    "    histogram2d(Wxx,Wyy,bins=200)#,aspect_ratio=1.0)\n",
    "    quiver!(x, y, quiver=(u_cur, v_cur));\n",
    "    scatter!([meanW[xind]],[meanW[yind]],leg=false)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current vector field\n",
    "xind = 6\n",
    "yind = 4\n",
    "Warray = gridarray(Wminf,xind,yind,Nx,Ny,Lx,Ly)\n",
    "npts = length(Warray[1,:])\n",
    "currents = zeros(Nweights,npts)\n",
    "for i=1:npts\n",
    "    currents[:,i] = O'*currentvec(unflat(O*Warray[:,i]),meanw,x_train,y_train,η)\n",
    "end\n",
    "x = Warray[xind,:];\n",
    "y = Warray[yind,:];\n",
    "mynorm = 3*sqrt(4*Lx*Ly/(Nx*Ny))/maximum(abs.(currents[[xind yind],:]))\n",
    "u_cur = mynorm*currents[xind,:];\n",
    "v_cur = mynorm*currents[yind,:];\n",
    "\n",
    "histogram2d(W_ss[trans:end,xind],W_ss[trans:end,yind],bins=200)\n",
    "quiver!(x, y, quiver=(u_cur, v_cur));\n",
    "scatter!([meanW[xind]],[meanW[yind]],leg=false)\n",
    "\n",
    "(xx,yy,Imask,xmask,ymask) = makegrid(Wx,Wy,meanW,xind,yind)\n",
    "meanXY = meanW[[xind yind]]\n",
    "midx = Int((length(xx)-1)/2)\n",
    "midy = Int((length(yy)-1)/2)\n",
    "\n",
    "# contours of loss\n",
    "contour!(xx,yy,Flossxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
