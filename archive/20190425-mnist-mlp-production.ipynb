{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Statistics, LinearAlgebra, Base.Iterators, Random, StatsBase, JLD\n",
    "ENV[\"COLUMNS\"] = 80\n",
    "ARRAY = KnetArray{Float64}\n",
    "XSIZE=784   # input dimension\n",
    "HIDDENSIZE=32 # hidden layer dimension\n",
    "YSIZE=10    # output dimension\n",
    "BATCHSIZE=10 # minibatch size\n",
    "LAMBDA=1e-2 # regularization parameter\n",
    "#LAMBDA=1e-10\n",
    "#LR=1e-1     # learning rate\n",
    "LR=1e-2     # learning rate\n",
    "#MITER=10^4  # iterations for finding minimum\n",
    "MITER=10^5  # iterations for finding minimum\n",
    "DITER=10^5  # iterations for diffusion tensor\n",
    "CITER=10^7  # iterations for covariance trajectory \n",
    "CFREQ=10^2  # keep every CFREQ points on trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define regularized linear model with softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(w,x) = reshape(w[HIDDENSIZE*(XSIZE+1)+1:end],YSIZE,HIDDENSIZE)*max.(0,reshape(w[1:HIDDENSIZE*XSIZE],HIDDENSIZE,XSIZE)*reshape(x,XSIZE,:) .+ w[HIDDENSIZE*XSIZE+1:HIDDENSIZE*(XSIZE+1)])\n",
    "loss(w,x,y;λ=LAMBDA) = nll(pred(w,x), y) + (λ/2) * sum(abs2,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data/mnist.jl\"))\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "atrn,atst = ARRAY(xtrn), ARRAY(xtst) # GPU copies for batch training\n",
    "println.(summary.((xtrn,ytrn,xtst,ytst,atrn,atst)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatching for SGD-I, i.e. with replacement. Knet.minibatch can't do this, we define new struct\n",
    "struct MB; x; y; n; end\n",
    "Base.Iterators.IteratorSize(::Type{MB}) = Base.IsInfinite() # need this for collect to work\n",
    "Base.iterate(d::MB, s...)=(r = rand(1:length(d.y),d.n); ((ARRAY(mat(d.x)[:,r]), d.y[r]), true))\n",
    "dtrn = MB(xtrn, ytrn, BATCHSIZE)\n",
    "println.(summary.(first(dtrn)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,MITER,BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum without minibatching\n",
    "# ~50 iters/sec, converges in 3 mins to \n",
    "# 0.267218 for LAMBDA=1e-4 (err=0.25, reg=0.02)\n",
    "# 0.344490 for LAMBDA=1e-3 (err=0.29, reg=0.05)\n",
    "# 0.558482 for LAMBDA=1e-2 (err=0.41, reg=0.15)\n",
    "wminfile = \"MLP-wmin-$LAMBDA-$MITER.jld2\"\n",
    "if !isfile(wminfile)\n",
    "    wmin = Param(ARRAY(LAMBDA*rand((XSIZE+1)*HIDDENSIZE + HIDDENSIZE*YSIZE)))\n",
    "    args = repeat([(wmin,atrn,ytrn)],MITER)\n",
    "    Knet.gc()\n",
    "    losses = collect(progress(adam(loss,args)))\n",
    "    Knet.save(wminfile, \"wmin\", wmin, \"losses\", losses)\n",
    "else\n",
    "    wmin, losses = Knet.load(wminfile, \"wmin\", \"losses\");\n",
    "end\n",
    "@show summary(wmin)\n",
    "losses[end-4:end]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println.((\n",
    "(loss(wmin,atrn,ytrn),nll(pred(wmin,atrn),ytrn),(LAMBDA/2)*sum(abs2,wmin)),\n",
    "(loss(wmin,atst,ytst),nll(pred(wmin,atst),ytst),(LAMBDA/2)*sum(abs2,wmin)),\n",
    "(accuracy(pred(wmin,atrn),ytrn),accuracy(pred(wmin,atst),ytst))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "∇loss = grad(loss)\n",
    "∇lossi(w,x,y,i) = ∇loss(w,x,y)[i]\n",
    "∇∇lossi = grad(∇lossi)\n",
    "w = value(wmin)\n",
    "n = length(w)\n",
    "∇lossi(w,atrn,ytrn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian of loss around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessian(loss,w,x,y)\n",
    "    ∇loss = grad(loss)\n",
    "    ∇lossi(w,x,y,i) = ∇loss(w,x,y)[i]\n",
    "    ∇∇lossi = grad(∇lossi)\n",
    "    w = value(w)\n",
    "    n = length(w)\n",
    "#    h = similar(w,n,n)\n",
    "    h = zeros(length(w),length(w))\n",
    "    for i in progress(1:n)\n",
    "        h[:,i] .= ARRAY(vec(∇∇lossi(w,x,y,i)))\n",
    "    end\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hessian: ~5 mins\n",
    "hessfile = \"MLP-hess-$LAMBDA.jld\"\n",
    "if !isfile(hessfile)\n",
    "    hmin = hessian(loss,wmin,atrn,ytrn)\n",
    "    save(hessfile,\"h\",hmin)\n",
    "else\n",
    "    hmin = load(hessfile,\"h\")\n",
    "end\n",
    "println.((summary(hmin),extrema(Array(hmin)),norm(hmin),norm(hmin-hmin')));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heigfile = \"MLP-nosym_heig-$LAMBDA.jld2\"\n",
    "H = Array(hmin)\n",
    "if !isfile(heigfile)\n",
    "#    @time eigenH = eigen(Symmetric(H)) # ~53s\n",
    "    @time eigenH = eigen(H) # ~53s\n",
    "    Knet.save(heigfile,\"eigenH\",eigenH)\n",
    "else\n",
    "    eigenH = Knet.load(heigfile,\"eigenH\")\n",
    "end\n",
    "eigenH.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(real.(eigenH.values)) |> dump\n",
    "scatter(real.(eigenH.values),xaxis=:log10,yaxis=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(loss,w,x,y;iters=DITER,lr=LR,batchsize=BATCHSIZE)\n",
    "    ∇loss = grad(loss)\n",
    "    grad0 = Array(∇loss(w, ARRAY(x), y))\n",
    "    data = MB(x,y,batchsize)\n",
    "    grads = zeros(length(w), iters)\n",
    "    for (i,d) in progress(enumerate(take(data,iters)))\n",
    "        grads[:,i] .= Array(∇loss(w,d...))\n",
    "    end\n",
    "    prefac = (lr^2)/(2iters)\n",
    "    grads = grad0 .- grads\n",
    "    @time v = prefac * (grads * grads')\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,LR,BATCHSIZE,DITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtfile = \"MLP-dt-$LAMBDA-$LR-$BATCHSIZE-$DITER.jld2\"\n",
    "if !isfile(dtfile)\n",
    "    Knet.gc()\n",
    "    D = diffusiontensor(loss,wmin,xtrn,ytrn) # ~700 iters/sec\n",
    "    Knet.save(dtfile,\"D\",D)\n",
    "else\n",
    "    D = Knet.load(dtfile,\"D\")\n",
    "end\n",
    "summarystats(vec(D)) |> dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record trajectory with SGD starting at minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,LR,BATCHSIZE,CITER,CFREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory of w starting from wmin recorded after each update: \n",
    "# ~1000 updates/sec, ~16 secs total\n",
    "trajfile = \"MLP-traj-$LAMBDA-$LR-$BATCHSIZE-$CITER-$CFREQ.jld2\"\n",
    "if !isfile(trajfile)\n",
    "    w = Param(ARRAY(value(wmin)))\n",
    "    data = MB(xtrn,ytrn,BATCHSIZE)\n",
    "    d = take(data,CITER)\n",
    "    W = zeros(eltype(w),length(w),div(CITER,CFREQ))\n",
    "    f(x,y) = loss(w,x,y)\n",
    "    Knet.gc()\n",
    "    i = 0\n",
    "    for t in progress(sgd(f,d; lr=LR))\n",
    "        i += 1; (div,rem)=divrem(i,CFREQ)\n",
    "        if rem == 0\n",
    "            W[:,div] = Array(vec(w))\n",
    "        end\n",
    "    end\n",
    "    Knet.save(trajfile,\"W\",W)\n",
    "else\n",
    "    W = Knet.load(trajfile,\"W\")\n",
    "end\n",
    "summary(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot losses on whole dataset, first steps seem transient, ~10 secs\n",
    "r = 1:100:size(W,2)\n",
    "@time plot(r, [loss(ARRAY(W[:,i]),atrn,ytrn) for i in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr1,rr2 = rand(1:size(W,1)),rand(1:size(W,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory of two random dimensions\n",
    "@show rr1,rr2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "if norm(W[rr1,:]) > 0 && norm(W[rr2,:]) > 0\n",
    "    histogram2d(W[rr1,:],W[rr2,:],background_color=\"black\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training seems to converge to a slightly worse spot\n",
    "w0 = Array(value(wmin))\n",
    "μ = mean(W[:,2500:end],dims=2)\n",
    "w1 = W[:,end]\n",
    "@show norm(w0), norm(μ), norm(w0 - μ)\n",
    "@show extrema(w0), extrema(μ), extrema(w0 - μ)\n",
    "@show mean(abs.(w0 - μ) .> 0.01)\n",
    "@show loss(w0,xtrn,ytrn)\n",
    "@show loss(μ,xtrn,ytrn)\n",
    "@show loss(w1,xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance of SGD trajectory around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wstable = W[:,2500:end];  @show summary(Wstable)\n",
    "Wstable = W\n",
    "μ = mean(Wstable,dims=2); @show summary(μ)\n",
    "Wzero = Wstable .- μ;     @show summary(Wzero)\n",
    "Σ = (Wzero * Wzero') / size(Wzero,2)\n",
    "@show summary(Σ)\n",
    "@show norm(Σ)\n",
    "@show extrema(Σ)\n",
    "@show norm(diag(Σ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for convergence\n",
    "n2 = div(size(W,2),2)\n",
    "w1 = W[:,1:n2]\n",
    "w2 = W[:,1+n2:end]\n",
    "w1 = w1 .- mean(w1,dims=2)\n",
    "w2 = w2 .- mean(w2,dims=2)\n",
    "Σ1 = (w1 * w1') / size(w1,2)\n",
    "Σ2 = (w2 * w2') / size(w2,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variances (diagonal elements) converge\n",
    "norm(diag(Σ1)),norm(diag(Σ2)),norm(diag(Σ1)-diag(Σ2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The off diagonal elements are still not there\n",
    "norm(Σ1),norm(Σ2),norm(Σ1-Σ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Einstein relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.((H,D,Σ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = H*Σ + Σ*H\n",
    "b = (2/LR)*D\n",
    "norm(a),norm(b),norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance eigenspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46 sec\n",
    "@time eigenΣ = eigen(Σ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ΛΣ = eigenΣ.values; O = eigenΣ.vectors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the eigenvectors/values are OK\n",
    "# norm(ΛΣ[end]*O[:,end]),norm(Σ*O[:,end]-ΛΣ[end]*O[:,end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the trajectory to the eigenbasis\n",
    "Weig = O'*W;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check Einstein relation in top Neig eigen-directions of Σ\n",
    "Neig=100\n",
    "Or = O[:,end-Neig+1:end];\n",
    "aa = Or'*a*Or\n",
    "bb = Or'*b*Or\n",
    "norm(aa),norm(bb)/norm(aa),norm(aa-bb)/norm(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa[1:10,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb[1:10,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick two eigen directions\n",
    "Nweights = size(W,1)\n",
    "Xid = Nweights\n",
    "Yid = Nweights-1\n",
    "\n",
    "#O = eigvecs(Σ);\n",
    "\n",
    "#W_ss = W*O; # sample weights are row vectors\n",
    "Wx = Weig[Xid,:]\n",
    "Wy = Weig[Yid,:]\n",
    "\n",
    "#COV_ss = O'*Σ*O\n",
    "#COV_xy_inv = inv(COV_ss[[Xid,Yid],[Xid,Yid]])\n",
    "COV_xy_inv = Diagonal([1/ΛΣ[Xid],1/ΛΣ[Yid]]) + zeros(2,2)\n",
    "μeig = O'*μ\n",
    "W0eig = O'*w0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myhplot = histogram2d(Wx,Wy\n",
    "    ,bins=100\n",
    "    ,aspect_ratio=1\n",
    "    ,background_color=\"black\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(myhplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a grid enclosing the steady-state trajectory\n",
    "minmaxdiff(t) = maximum(t)-minimum(t)\n",
    "\n",
    "function makegrid(xvec,yvec,mean,xindex,yindex;Nx=50,Ny=50,zoom=0.35)\n",
    "    Lx,Ly = minmaxdiff(xvec),minmaxdiff(yvec)\n",
    "    xrange,yrange = zoom*Lx,zoom*Ly\n",
    "    dx,dy = xrange/Nx,yrange/Ny\n",
    "    x = collect(-xrange:dx:xrange) .+ mean[xindex]\n",
    "    y = collect(-yrange:dy:yrange) .+ mean[yindex]\n",
    "\n",
    "    # some mumbo-jumbo for calculating weights corresponding to grid points\n",
    "    Identity = Diagonal(ones(Nweights,Nweights)); # unit matrix\n",
    "    xmask = Identity[:,xindex];\n",
    "    ymask = Identity[:,yindex];\n",
    "    Imask = Identity - xmask*xmask' - ymask*ymask' # set two diagonal elements to zero\n",
    "    return (x,y,Imask,xmask,ymask)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,Imask,xmask,ymask) = makegrid(Wx,Wy,W0eig,Xid,Yid;Nx=12,Ny=12,zoom=0.5)\n",
    "#(x,y,Imask,xmask,ymask) = makegrid(Wx,Wy,μeig,Xid,Yid;Nx=4,Ny=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanXY = W0eig[[Xid Yid]]\n",
    "# Contours of the fit mv-Gaussian\n",
    "ffit(s,t) = -(([s t]-meanXY)*COV_xy_inv*([s t]-meanXY)')[1]\n",
    "Ffit(s,t) = 250*ffit(s,t)/ffit(x[end],y[end])\n",
    "contour!(x,y,Ffit\n",
    "    ,linestyle=:dash\n",
    "    ,levels=10\n",
    "    ,linewidth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contours of loss\n",
    "midx = Int((length(x)-1)/2)\n",
    "midy = Int((length(y)-1)/2)\n",
    "#fexp(s,t) = 1e5*(loss(O*(Imask*W0eig + s*xmask + t*ymask),xtrn,ytrn) - loss(w0,xtrn,ytrn))\n",
    "fexp(s,t) = 1e5*(loss(O*(Imask*μeig + s*xmask + t*ymask),xtrn,ytrn) - loss(w0,xtrn,ytrn))\n",
    "logfexpmidp1 = log(1+fexp(x[midx],y[midy]))\n",
    "Flossxy(s,t) = 1e2*(log((1+fexp(s,t))) - logfexpmidp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flossxy(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N,N-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time contour!(x,y,Flossxy\n",
    "#contour!(x,y,fexp\n",
    "    ,levels=10\n",
    "    ,linewidth=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
