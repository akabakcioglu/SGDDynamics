{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Statistics, LinearAlgebra, Base.Iterators, Random, StatsBase\n",
    "ENV[\"COLUMNS\"] = 80\n",
    "ARRAY = KnetArray{Float64}\n",
    "XSIZE=784   # input dimension\n",
    "YSIZE=10    # output dimension\n",
    "BATCHSIZE=100 # minibatch size\n",
    "LAMBDA=1e-2 # regularization parameter\n",
    "LR=1e-1     # learning rate\n",
    "MITER=10^4  # iterations for finding minimum\n",
    "DITER=10^5  # iterations for diffusion tensor\n",
    "CITER=10^6  # iterations for covariance trajectory \n",
    "CFREQ=10^1  # keep every CFREQ points on trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define regularized linear model with softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(w,x) = reshape(w,YSIZE,XSIZE) * reshape(x,XSIZE,:)\n",
    "loss(w,x,y;λ=LAMBDA) = nll(pred(w,x), y) + (λ/2) * sum(abs2,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data/mnist.jl\"))\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "atrn,atst = ARRAY(xtrn), ARRAY(xtst) # GPU copies for batch training\n",
    "println.(summary.((xtrn,ytrn,xtst,ytst,atrn,atst)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatching for SGD-I, i.e. with replacement. Knet.minibatch can't do this, we define new struct\n",
    "struct MB; x; y; n; end\n",
    "Base.Iterators.IteratorSize(::Type{MB}) = Base.IsInfinite() # need this for collect to work\n",
    "Base.iterate(d::MB, s...)=(r = rand(1:length(d.y),d.n); ((ARRAY(mat(d.x)[:,r]), d.y[r]), true))\n",
    "dtrn = MB(xtrn, ytrn, BATCHSIZE)\n",
    "println.(summary.(first(dtrn)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,MITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum without minibatching\n",
    "# ~50 iters/sec, converges in 3 mins to \n",
    "# 0.267218 for LAMBDA=1e-4 (err=0.25, reg=0.02)\n",
    "# 0.344490 for LAMBDA=1e-3 (err=0.29, reg=0.05)\n",
    "# 0.558482 for LAMBDA=1e-2 (err=0.41, reg=0.15)\n",
    "wminfile = \"wmin-$LAMBDA-$MITER.jld2\"\n",
    "if !isfile(wminfile)\n",
    "    wmin = Param(ARRAY(zeros(XSIZE*YSIZE)))\n",
    "    args = repeat([(wmin,atrn,ytrn)],MITER)\n",
    "    Knet.gc()\n",
    "    losses = collect(progress(adam(loss,args)))\n",
    "    Knet.save(wminfile, \"wmin\", wmin, \"losses\", losses)\n",
    "else\n",
    "    wmin, losses = Knet.load(wminfile, \"wmin\", \"losses\");\n",
    "end\n",
    "@show summary(wmin)\n",
    "losses[end-4:end]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println.((\n",
    "(loss(wmin,atrn,ytrn),nll(pred(wmin,atrn),ytrn),(LAMBDA/2)*sum(abs2,wmin)),\n",
    "(loss(wmin,atst,ytst),nll(pred(wmin,atst),ytst),(LAMBDA/2)*sum(abs2,wmin)),\n",
    "(accuracy(pred(wmin,atrn),ytrn),accuracy(pred(wmin,atst),ytst))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian of loss around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessian(loss,w,x,y)\n",
    "    ∇loss = grad(loss)\n",
    "    ∇lossi(w,x,y,i) = ∇loss(w,x,y)[i]\n",
    "    ∇∇lossi = grad(∇lossi)\n",
    "    w = value(w)\n",
    "    n = length(w)\n",
    "    h = similar(w,n,n)\n",
    "    for i in progress(1:n)\n",
    "        h[:,i] .= vec(∇∇lossi(w,x,y,i))\n",
    "    end\n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hessian: ~5 mins\n",
    "hessfile = \"hess-$LAMBDA.jld2\"\n",
    "if !isfile(hessfile)\n",
    "    Knet.gc()\n",
    "    hmin = hessian(loss,wmin,atrn,ytrn)\n",
    "    Knet.save(hessfile,\"h\",hmin)\n",
    "else\n",
    "    hmin = Knet.load(hessfile,\"h\")\n",
    "end\n",
    "println.((summary(hmin),extrema(Array(hmin)),norm(hmin),norm(hmin-hmin')));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heigfile = \"heig-$LAMBDA.jld2\"\n",
    "H = Array(hmin)\n",
    "if !isfile(heigfile)\n",
    "    @time eigenH = eigen(Symmetric(H)) # ~53s\n",
    "    Knet.save(heigfile,\"eigenH\",eigenH)\n",
    "else\n",
    "    eigenH = Knet.load(heigfile,\"eigenH\")\n",
    "end\n",
    "eigenH.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(eigenH.values) |> dump\n",
    "plot(eigenH.values, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(loss,w,x,y;iters=DITER,lr=LR,batchsize=BATCHSIZE)\n",
    "    ∇loss = grad(loss)\n",
    "    grad0 = Array(∇loss(w, ARRAY(x), y))\n",
    "    data = MB(x,y,batchsize)\n",
    "    grads = zeros(length(w), iters)\n",
    "    for (i,d) in progress(enumerate(take(data,iters)))\n",
    "        grads[:,i] .= Array(∇loss(w,d...))\n",
    "    end\n",
    "    prefac = (lr^2)/(2iters)\n",
    "    grads = grad0 .- grads\n",
    "    @time v = prefac * (grads * grads')\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,LR,BATCHSIZE,DITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtfile = \"dt-$LAMBDA-$LR-$BATCHSIZE-$DITER.jld2\"\n",
    "if !isfile(dtfile)\n",
    "    Knet.gc()\n",
    "    D = diffusiontensor(loss,wmin,xtrn,ytrn) # ~700 iters/sec\n",
    "    Knet.save(dtfile,\"D\",D)\n",
    "else\n",
    "    D = Knet.load(dtfile,\"D\")\n",
    "end\n",
    "summarystats(vec(D)) |> dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record trajectory with SGD starting at minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA,LR,BATCHSIZE,CITER,CFREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory of w starting from wmin recorded after each update: \n",
    "# ~1000 updates/sec, ~16 secs total\n",
    "trajfile = \"traj-$LAMBDA-$LR-$BATCHSIZE-$CITER-$CFREQ.jld2\"\n",
    "if !isfile(trajfile)\n",
    "    w = Param(ARRAY(value(wmin)))\n",
    "    data = MB(xtrn,ytrn,BATCHSIZE)\n",
    "    d = take(data,CITER)\n",
    "    W = zeros(eltype(w),length(w),div(CITER,CFREQ))\n",
    "    f(x,y) = loss(w,x,y)\n",
    "    Knet.gc()\n",
    "    i = 0\n",
    "    for t in progress(sgd(f,d; lr=LR))\n",
    "        i += 1; (div,rem)=divrem(i,CFREQ)\n",
    "        if rem == 0\n",
    "            W[:,div] = Array(vec(w))\n",
    "        end\n",
    "    end\n",
    "    Knet.save(trajfile,\"W\",W)\n",
    "else\n",
    "    W = Knet.load(trajfile,\"W\")\n",
    "end\n",
    "summary(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot losses on whole dataset, first steps seem transient, ~10 secs\n",
    "r = 1:100:size(W,2)\n",
    "@time plot(r, [loss(ARRAY(W[:,i]),atrn,ytrn) for i in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(W[r2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory of two random dimensions\n",
    "@show r1,r2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "if norm(W[r1,:]) > 0 && norm(W[r2,:]) > 0\n",
    "    histogram2d(W[r1,:],W[r2,:])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training seems to converge to a slightly worse spot\n",
    "w0 = Array(value(wmin))\n",
    "μ = mean(W[:,2500:end],dims=2)\n",
    "w1 = W[:,end]\n",
    "@show norm(w0), norm(μ), norm(w0 - μ)\n",
    "@show extrema(w0), extrema(μ), extrema(w0 - μ)\n",
    "@show mean(abs.(w0 - μ) .> 0.01)\n",
    "@show loss(w0,xtrn,ytrn)\n",
    "@show loss(μ,xtrn,ytrn)\n",
    "@show loss(w1,xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance of SGD trajectory around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wstable = W[:,2500:end];  @show summary(Wstable)\n",
    "Wstable = W\n",
    "μ = mean(Wstable,dims=2); @show summary(μ)\n",
    "Wzero = Wstable .- μ;     @show summary(Wzero)\n",
    "Σ = (Wzero * Wzero') / size(Wzero,2)\n",
    "@show summary(Σ)\n",
    "@show norm(Σ)\n",
    "@show extrema(Σ)\n",
    "@show norm(diag(Σ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for convergence\n",
    "n2 = div(size(W,2),2)\n",
    "w1 = W[:,1:n2]\n",
    "w2 = W[:,1+n2:end]\n",
    "w1 = w1 .- mean(w1,dims=2)\n",
    "w2 = w2 .- mean(w2,dims=2)\n",
    "Σ1 = (w1 * w1') / size(w1,2)\n",
    "Σ2 = (w2 * w2') / size(w2,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variances (diagonal elements) converge\n",
    "norm(diag(Σ1)),norm(diag(Σ2)),norm(diag(Σ1)-diag(Σ2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The off diagonal elements are still not there\n",
    "norm(Σ1),norm(Σ2),norm(Σ1-Σ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.((H,D,Σ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = H*Σ + Σ*H\n",
    "b = (2/LR)*D\n",
    "norm(a),norm(b),norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm(a),norm(b),norm(a-b)\n",
    "# (0.001831031218512692, 0.0015956482650563955, 0.000668750876334433): CITER=1M CFREQ=100\n",
    "# (0.0017456054525856169, 0.0015956482650563955, 0.00044092592553571534): CITER=1M CFREQ=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[3000:3004,3000:3004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[3000:3004,3000:3004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a./b)[3000:3004,3000:3004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarystats(vec(abs.(a))) |> dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = a; a0[abs.(a0) .< 1e-7] .= 0\n",
    "b0 = b; b0[abs.(b0) .< 1e-7] .= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(a0), norm(b0), norm(a0-b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check convergence of covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigma(W)\n",
    "    μ = mean(W,dims=2)\n",
    "    W0 = W .- μ\n",
    "    Σ = (W0 * W0') / size(W0,2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [ sigma(W[:,1:(i*1000)]) for i in (1,2,5,10,20,50,100) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 2:length(sigmas)\n",
    "   println((norm(sigmas[i-1]),norm(sigmas[i]),norm(sigmas[i]-sigmas[i-1])))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1K,2K,...,10K sampling every 100 up to 1M convergence\n",
    "(0.17517119848767196, 0.18735112177809712, 0.1736369823475756)\n",
    "(0.18735112177809712, 0.18194022797202322, 0.13163000956991822)\n",
    "(0.18194022797202322, 0.17754343048529778, 0.10970021736453625)\n",
    "(0.17754343048529778, 0.16902882189691706, 0.08681261945879391)\n",
    "(0.16902882189691706, 0.16118998631852746, 0.07373054738895173)\n",
    "(0.16118998631852746, 0.15395757281329958, 0.06383283177104303)\n",
    "(0.15395757281329958, 0.14673395708896458, 0.05427855979560519)\n",
    "(0.14673395708896458, 0.14047011468580411, 0.04757881795972907)\n",
    "(0.14047011468580411, 0.13584321137382668, 0.04594090264182607)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1K,2K,5K,10K,20K,50K,100K sampling every 10 up to 1M \n",
    "(0.09419547449556358, 0.12122309088637423, 0.11268632188794234)\n",
    "(0.12122309088637423, 0.15926002834426348, 0.15786197264298812)\n",
    "(0.15926002834426348, 0.17998457741465182, 0.16622733785950075)\n",
    "(0.17998457741465182, 0.18679671394160588, 0.1732145346986742)\n",
    "(0.18679671394160588, 0.16400614726671942, 0.17491052732529933)\n",
    "(0.16400614726671942, 0.1335983636197007, 0.1283291677336581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 2:length(sigmas)\n",
    "   println((norm(diag(sigmas[i-1])),norm(diag(sigmas[i])),norm(diag(sigmas[i]-sigmas[i-1]))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(norm.(diag.(sigmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sort(diag(H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian (numeric check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(w) ≈ f(wmin) + (w-wmin)' g + 1/2 (w-wmin)' H (w-wmin)\n",
    "# Gradient at wmin is ≈0, so the middle term can be assumed 0\n",
    "df = @diff loss(wmin,atrn,ytrn)\n",
    "J = vec(grad(df, wmin)); @show summary(J)\n",
    "@show norm(J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test approx at ~1 distance around wmin\n",
    "# adding first order term does not make much difference as expected\n",
    "wrnd = randn!(similar(wmin)) / sqrt(length(wmin))\n",
    "lossw(w) = loss(w,atrn,ytrn)\n",
    "@show lossw(wmin)\n",
    "@show lossw(wmin + wrnd)\n",
    "@show lossw(wmin) + 0.5 * wrnd' * hmin * wrnd\n",
    "@show lossw(wmin) + J' * wrnd + 0.5 * wrnd' * hmin * wrnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the diffusion tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deigfile = \"deig-$LAMBDA-$LR-$BATCHSIZE-$DITER.jld2\"\n",
    "if !isfile(deigfile)\n",
    "    @time eigenD = eigen(Symmetric(D)) # ~53s\n",
    "    Knet.save(deigfile,\"eigenD\",eigenD)\n",
    "else\n",
    "    eigenD = Knet.load(deigfile,\"eigenD\")\n",
    "end\n",
    "eigenD.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(eigenD.values .+ 1e-19, yscale=:log10) |> display\n",
    "summarystats(eigenD.values) |> dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check convergence of variance in SGD trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE,LR,LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w  = ARRAY(value(wmin))\n",
    "w1 = w .+ 0\n",
    "w2 = w .* w\n",
    "nw = 1\n",
    "pw = Param(w)\n",
    "data = MB(xtrn,ytrn,BATCHSIZE)\n",
    "f(x,y) = loss(pw,x,y,\\lambda=LAMBDA)\n",
    "\n",
    "function wvar(w)\n",
    "    global nw, w1, w2\n",
    "    nw += 1\n",
    "    w1 .+= w\n",
    "    w2 .+= w .* w\n",
    "    var = w2 / nw - (w1 .* w1) / (nw * nw) # E[x^2] - E[x]^2\n",
    "    norm(var)\n",
    "end\n",
    "\n",
    "c = collect(progress(wvar(w) for wloss in sgd(f, take(data,500000), lr=LR)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(c, xscale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(w),norm(w.*w),norm(w1),norm(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues of the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceigfile = \"ceig07.jld2\"\n",
    "if !isfile(ceigfile)\n",
    "    @time eigenC = eigen(Symmetric(Σ)) # ~53s\n",
    "    Knet.save(ceigfile,\"eigenC\",eigenC)\n",
    "else\n",
    "    eigenC = Knet.load(ceigfile,\"eigenC\")\n",
    "end\n",
    "eigenC.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(eigenC.values, yscale=:log10) |> display\n",
    "summarystats(eigenC.values) |> dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Σ is not positive definite, MLE fails because Hessian=inverse(Σ)\n",
    "# Note that this is not the same as the loss Hessian defined below, it is the distribution Hessian!\n",
    "# using Distributions\n",
    "# b = fit_mle(MvNormal, Wstable)\n",
    "@show sum(diag(Σ) .== 0) # 670 dims do not move at all!\n",
    "@show sum(wmin .== 0) # these stay at 0 in the w matrix.\n",
    "findall(diag(Σ) .== 0) == findall(Array(wmin) .== 0) # to make sure they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionTensor Convergence \n",
    "(with LAMBDA=0.0001,LR=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm(d100),norm(d1000),norm(d100-d1000)         #(9.006901905269366e-5, 7.966566524654371e-5, 4.746245825247884e-5)\n",
    "#norm(d1000),norm(d2000),norm(d1000-d2000)       #(7.966566524654371e-5, 7.976183314696048e-5, 1.8908596356951573e-5)\n",
    "#norm(d4000),norm(d2000),norm(d4000-d2000)       #(8.024754500933944e-5, 7.976183314696048e-5, 1.3446098748867312e-5)\n",
    "#norm(d10000),norm(d4000),norm(d10000-d4000)     #(7.942869188760434e-5, 8.024754500933944e-5, 8.963487531775732e-6)\n",
    "#norm(d10000),norm(d20000),norm(d20000-d10000)   #(7.955107659141219e-5, 7.976461525637195e-5, 5.902290704618786e-6)\n",
    "#norm(d20000),norm(d50000),norm(d50000-d20000)   #(7.976461525637195e-5, 7.971793474706985e-5, 3.962945418293533e-6)\n",
    "#norm(d50000),norm(d100000),norm(d100000-d50000) #(7.971793474706985e-5, 7.978241325281978e-5, 2.739184868054573e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plots for trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to converge to a slightly different point?\n",
    "# Interesting patterns: staircase, globe, H shaped\n",
    "@show r1,r2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "scatter(W[r1,1:end],W[r2,1:end])\n",
    "scatter!(W[r1,1:10], W[r2,1:10],mc=:red) # mark beginning with red\n",
    "scatter!(W[r1,end-9:end],W[r2,end-9:end],mc=:yellow) # mark end with yellow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
