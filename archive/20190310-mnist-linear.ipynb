{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Statistics, LinearAlgebra, Base.Iterators, Random\n",
    "ENV[\"COLUMNS\"]=40\n",
    "ARRAY = KnetArray{Float64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data/mnist.jl\"))\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "gtrn,gtst = ARRAY(xtrn), ARRAY(xtst) # GPU copies for batch training\n",
    "println.(summary.((xtrn,ytrn,xtst,ytst,gtrn,gtst)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define regularized linear model with softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use simple linear model with regularization (otherwise w blows up)\n",
    "struct Linear; w; λ; end    # new type\n",
    "Linear(w;λ=LAMBDA) = Linear(w,λ)  # constructor\n",
    "Linear(i::Int,o::Int;λ=LAMBDA) = Linear(param(o,i,atype=ARRAY),λ)  # constructor\n",
    "(f::Linear)(x) = reshape(f.w,10,:) * mat(x)   # predict\n",
    "(f::Linear)(x,y) = nll(f(x),y) + (f.λ/2) * sum(abs2,f.w)   # loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum without minibatching ~50 iters/sec\n",
    "if !isfile(\"fmin03.jld2\")\n",
    "    fmin = Linear(784,10)\n",
    "    data = repeat([(gtrn,ytrn)],10000)\n",
    "    Knet.gc()\n",
    "    losses = collect(progress(adam(fmin,data)))\n",
    "    Knet.save(\"fmin03.jld2\", \"fmin\", fmin, \"losses\", losses)\n",
    "else\n",
    "    fmin, losses = Knet.load(\"fmin03.jld2\", \"fmin\", \"losses\");\n",
    "end\n",
    "@show summary(fmin.w)\n",
    "losses[end-10:end]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(losses, ylim=(.26,.28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatching for SGD-II, shuffle=true corresponds to SGD-II, i.e. without replacement\n",
    "# dtrn1 = minibatch(xtrn,ytrn,100;xtype=ARRAY,shuffle=true)\n",
    "# x1,y1 = first(dtrn1)\n",
    "# println.(summary.((dtrn1,x1,y1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatching for SGD-I, i.e. with replacement. Knet.minibatch can't do this, we define new struct\n",
    "struct MB; x; y; n; end\n",
    "Base.Iterators.IteratorSize(::MB) = Base.IsInfinite()\n",
    "Base.iterate(d::MB, s...)=(r = rand(1:length(d.y),d.n); ((ARRAY(d.x[:,r]), d.y[r]), true))\n",
    "dtrn = MB(mat(xtrn), ytrn, 100)\n",
    "println.(summary.((dtrn,first(dtrn)...)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record trajectory with SGD starting at minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory of w starting from wmin recorded after each update: ~1000 updates/sec\n",
    "f = deepcopy(fmin)\n",
    "f.w.opt = nothing  # We do not want to use Adam! => TODO: warn about this.\n",
    "d = take(dtrn,9999)\n",
    "W = zeros(eltype(f.w),length(f.w),1+length(d))\n",
    "i = 1\n",
    "W[:,i] = vec(Array(f.w))\n",
    "Knet.gc()\n",
    "for t in progress(sgd(f,d; lr=LR))\n",
    "    i += 1\n",
    "    W[:,i] = vec(Array(f.w))\n",
    "end\n",
    "@show summary(W);\n",
    "@show f.w.opt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot losses on whole dataset, first steps seem transient\n",
    "r = 1:10:size(W,2)\n",
    "@time plot(r, [Linear(ARRAY(W[:,i]))(gtrn,ytrn) for i in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory of two random dimensions\n",
    "# Seems to converge to a slightly different point?\n",
    "# Interesting patterns: staircase, globe, H shaped\n",
    "@show r1,r2 = rand(1:size(W,1)),rand(1:size(W,1))\n",
    "scatter(W[r1,1:end],W[r2,1:end])\n",
    "scatter!(W[r1,1:10], W[r2,1:10],mc=:red) # mark beginning with red\n",
    "scatter!(W[r1,end-9:end],W[r2,end-9:end],mc=:yellow) # mark end with yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training seems to converge to a slightly worse spot\n",
    "wmin = Array(vec(fmin.w.value))\n",
    "μ = mean(W[:,2500:end],dims=2)\n",
    "wlast = W[:,end]\n",
    "@show norm(wmin), norm(μ)\n",
    "@show norm(wmin - μ)\n",
    "@show extrema(wmin - μ)\n",
    "@show mean(abs.(wmin - μ) .> 0.01)\n",
    "@show Linear(wmin)(xtrn,ytrn)\n",
    "@show Linear(μ)(xtrn,ytrn)\n",
    "@show Linear(wlast)(xtrn,ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance of SGD trajectory around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wstable = W[:,2500:end];  @show summary(Wstable)\n",
    "μ = mean(Wstable,dims=2); @show summary(μ)\n",
    "Wzero = Wstable .- μ;     @show summary(Wzero)\n",
    "Σ = (Wzero * Wzero') / size(Wzero,2); @show summary(Σ)\n",
    "@show norm(Σ),extrema(Σ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Σ is not positive definite, MLE fails because Hessian=inverse(Σ)\n",
    "# Note that this is not the same as the loss Hessian defined below, it is the distribution Hessian!\n",
    "# using Distributions\n",
    "# b = fit_mle(MvNormal, Wstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"COLUMNS\"]=94\n",
    "Σ[3001:3005,3001:3005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show Σ == Σ'\n",
    "#@time eigenΣ = eigen(Symmetric(Σ))  # ~53s\n",
    "#eigenΣ.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(eigenΣ.values .+ 1e-16, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian of loss around minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessian(loss,w,x,y)\n",
    "    ∇loss = grad(loss)\n",
    "    ∇lossi(w,x,y,i) = ∇loss(w,x,y)[i]\n",
    "    ∇∇lossi = grad(∇lossi)\n",
    "    n = length(w)\n",
    "    h = similar(w,n,n)\n",
    "    for i in progress(1:n)\n",
    "        h[:,i] .= vec(∇∇lossi(w,x,y,i))\n",
    "    end\n",
    "    return h # Symmetric(Array(0.5*(h+h')))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hessian: ~6 mins\n",
    "if !isfile(\"hess03.jld2\")\n",
    "    Knet.gc()\n",
    "    wmin = fmin.w.value\n",
    "    loss(w,x,y) = Linear(w)(x,y)\n",
    "    hmin = hessian(loss,wmin,gtrn,ytrn)\n",
    "    Knet.save(\"hess03.jld2\",\"h\",hmin)\n",
    "else\n",
    "    hmin = Knet.load(\"hess03.jld2\",\"h\")\n",
    "end\n",
    "summary(hmin),norm(hmin),extrema(Array(hmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hmin[3001:3005,3001:3005])\n",
    "@show isapprox(hmin,hmin',rtol=0.2)\n",
    "@show isapprox(hmin,hmin',rtol=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Array(0.5*(hmin + hmin'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@time eigenH = eigen(Symmetric(H))  # ~53s\n",
    "#eigenH.values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigenH.values[4000:4010]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(eigenH.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian (numeric check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(w) ≈ f(wmin) + (w-wmin)' g + 1/2 (w-wmin)' H (w-wmin)\n",
    "# Gradient at wmin is ≈0, so the middle term can be assumed 0\n",
    "df = @diff fmin(gtrn,ytrn)\n",
    "J = vec(grad(df, fmin.w)); @show summary(J)\n",
    "norm(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sample points around wmin with distance ~ 1\n",
    "@show mean(sqrt.(sum(abs2, Wstable .- μ,dims=1)))\n",
    "@show mean(sqrt.(sum(abs2, Wstable .- vec(Array(wmin)), dims=1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently we do not need the 1/2 factor?\n",
    "# hmin and hgpu give identical results?\n",
    "# adding first order term does not make much difference as expected\n",
    "hgpu = KnetArray(H)\n",
    "@show norm(hmin), norm(hgpu), norm(hmin-hgpu)\n",
    "wmin = vec(fmin.w.value)\n",
    "wrnd = randn!(similar(wmin)) / sqrt(length(wmin))\n",
    "loss(w) = Linear(w)(gtrn,ytrn)\n",
    "@show loss(wmin)\n",
    "@show loss(wmin + wrnd)\n",
    "@show loss(wmin) + wrnd' * hgpu * wrnd\n",
    "@show loss(wmin) + wrnd' * hmin * wrnd\n",
    "@show loss(wmin) + J' * wrnd + wrnd' * hmin * wrnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusiontensor(loss,w,data,lr=LR) # lr=0.1 is default for sgd\n",
    "    ∇loss = grad(loss)\n",
    "    grads = [ ∇loss(w,x,y) for (x,y) in data ]\n",
    "    n,m = length(grads), mean(grads)\n",
    "    prefac = lr^2/(2n)\n",
    "    v = ARRAY(zeros(length(w),length(w)))\n",
    "    for g in progress(grads)\n",
    "        e=vec(m-g)\n",
    "        axpy!(prefac,e*e',v)\n",
    "    end\n",
    "    return Array(v)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmin = reshape(wmin,10,:)\n",
    "summary.((wmin,first(dtrn)...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute diffusion tensor ~24 secs/600 iter. should we go longer?\n",
    "Knet.gc()\n",
    "loss(w,x,y) = Linear(w)(x,y)\n",
    "D = diffusiontensor(loss,wmin,take(dtrn,600));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.((Σ,H,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show Σ == Σ'\n",
    "@show H == H'\n",
    "@show D == D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = H*Σ + Σ*H\n",
    "b = (2/LR)*D\n",
    "a ≈ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(a),norm(b),norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
