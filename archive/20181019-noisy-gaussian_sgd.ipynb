{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg,Statistics,Random,Printf,GZip,Knet,Plots,LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Range=3.0; # range of the x values for the target Gaussian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Incr = 0.3; # determines the number of samples from which we'll learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_std=0.1; # add noise on the Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data from which we'll learn the Gaussian function\n",
    "# obligatory arguments listed before \";\" while optional arguments come after \";\".\n",
    "function gen_noisy_gaussian(;range=1.0,noise=0.1)\n",
    "#    x = sort(randn(len)*range) # randn(): normal distributed \n",
    "    x = collect(-Range:Incr:Range)\n",
    "    \n",
    "#    y = exp.(-x.^2).*(-noise*2*(rand(length(x)).-0.5).+1) # fractional noise\n",
    "     y = exp.(-x.^2) + randn(length(x))*noise; # additive gaussian noise\n",
    "    return (x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train) = gen_noisy_gaussian(range=Range,noise=Noise_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train =length(x_train) # number of training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(x_train,[y_train,exp.(-x_train.^2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose for easier manipulation during training\n",
    "x_train = permutedims(x_train);\n",
    "y_train = permutedims(y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layersize = 50; # number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = 1\n",
    "# input = 1\n",
    "# hidden = 50\n",
    "# batchsize = 1\n",
    "# one layer: tahn.(w[hidden,input] * x[input,batchsize] .+ b[hidden,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(2); # Modify weight initialization w/o changing the training data after kernel resets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "w = [0.1*rand(Layersize,1),0.1*rand(Layersize,1),0.1*rand(1,Layersize),0.1*rand(1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions:\n",
    "# w[1]: (hidden x input) - input->hidden weights\n",
    "# x: (input x batchsize) - input\n",
    "# w[2]: (hidden x 1) - input->hidden bias\n",
    "# w[3]: (hidden x output) - hidden-> output weights\n",
    "# w[4]: (output x 1) - hidden->output bias\n",
    "\n",
    "function loss(w,x,y)\n",
    "    guesses = sum(w[3] * tanh.(w[1]*x.+w[2]) .+ w[4],dims=1) # w[1]=w, w[2]=w0, w[3]=w', w[4]=w0'\n",
    "    return mean(abs2,y-guesses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct the gradient-calculating function\n",
    "lossgradient = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = lossgradient(w,[x_train[1]],[y_train[1]])  # dw has dimnensions of w\n",
    "                                                # output is the gradient w.r.t. the corresponding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mytrain!(w;lr=0.1)\n",
    "    for n=1:N_train\n",
    "#    for n=1:randperm(N_train)\n",
    "        dw = lossgradient(w,[x_train[n]],[y_train[n]]);\n",
    "        for i=1:length(w)\n",
    "            for j=1:length(w[i])\n",
    "                w[i][j] -= lr*dw[i][j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 200000; # This needs to be determined by trial and error, depending on the data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.01;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect weights after each epoch in an array (trajectory)\n",
    "@time weights = [ deepcopy(mytrain!(w,lr=Learning_Rate)) for epoch=1:Nepoch ];  # copy only copies the top layer, does not descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot=collect(-Range:0.01:Range);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = permutedims(sum(w[3] * tanh.(w[1]*permutedims(xplot).+w[2]) .+ w[4],dims=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the converged function, the initial gaussian and the noisy training samples\n",
    "plot(xplot,[y_pred,exp.(-xplot.^2)]); scatter!(x_train,y_train,leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SamplingRate=10;\n",
    "\n",
    "x = collect(1:SamplingRate:Nepoch);\n",
    "\n",
    "y = [loss(weights[i],x_train,y_train) for i in x];\n",
    "\n",
    "plot(x,y;yscale=:log,xscale=:log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP HERE AND GUESS THE \"GOOD\" MINIMUM FROM THE PLATEAU IN THE LOSS CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngoodmin = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the predicted function after Ngoodmin epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = permutedims(sum(weights[Ngoodmin][3] * tanh.(weights[Ngoodmin][1]*permutedims(xplot).+weights[Ngoodmin][2]) .+ weights[Ngoodmin][4],dims=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the converged function, the initial gaussian and the noisy training samples\n",
    "plot(xplot,[y_pred,exp.(-xplot.^2)]); scatter!(x_train,y_train,leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the loss on (w0 -> wg), (wg -> w*) and (w0 -> w*) segments (wg is the \"good\" solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = collect(1:100);\n",
    "\n",
    "y = [loss(weights[1]+0.01*i*(weights[Ngoodmin]-weights[1]),x_train,y_train) for i in x];\n",
    "\n",
    "plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = collect(1:100);\n",
    "\n",
    "y = [loss(weights[Ngoodmin]+0.01*i*(weights[Nepoch]-weights[Ngoodmin]),x_train,y_train) for i in x];\n",
    "\n",
    "plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = collect(1:100);\n",
    "\n",
    "y = [loss(weights[1]+0.01*i*(weights[Nepoch]-weights[1]),x_train,y_train) for i in x];\n",
    "\n",
    "plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd steps\n",
    "deltaweights = [ vcat(weights[i][1],weights[i][2],weights[i][3]',weights[i][4])-vcat(weights[i-1][1],weights[i-1][2],weights[i-1][3]',weights[i-1][4]) for i=2:Nepoch ]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coarse sgd steps\n",
    "Ncoarse = 100;\n",
    "coarse_dw = [ sum(deltaweights[1+(i-1)*Ncoarse:i*Ncoarse]) for i=1:floor(Int,(Nepoch-1)/Ncoarse)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_normdw = [ v/norm(v) for v in coarse_dw];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_normdw = dot.(coarse_normdw,coarse_normdw');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(dot_normdw) # would be better if I could plot this heatmap in log-scale. I need to sample dw's accordingly.\n",
    "# Note that each increment in x and y axes corresponds to Ncoarse epochs (the \"good\" prediction is already there at 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nzoom = floor(Int,5*Ngoodmin/Ncoarse);\n",
    "heatmap(dot_normdw[1:Nzoom,1:Nzoom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to find the dimensionality of the GD trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function project_out(v,basis) # returns the component of v orthogonal to the support of orthonormal cols of \"basis\"\n",
    "    if length(basis)==0\n",
    "        return v\n",
    "    else\n",
    "        dots = v'*basis\n",
    "        return (v-sum(basis*diagm(0=>dots[:]),dims=2))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gd_support(gdsteps;minnorm=0.5) # minnorm = threshold beyond which perp. gradient component is considered new\n",
    "    mybasis = Array{Float64}(undef,length(gdsteps[1]),0) # records the basis vectors for the past steps\n",
    "    mystrides = []; # records the number of gd steps taken in the current manifold\n",
    "    nsteps = 1;\n",
    "    for v in gdsteps\n",
    "        vperp = project_out(v,mybasis)\n",
    "        if (norm(vperp) > minnorm)\n",
    "            mybasis = hcat(mybasis,vperp/norm(vperp))\n",
    "            mystrides = push!(mystrides,nsteps)\n",
    "            nsteps = 1\n",
    "        else\n",
    "            nsteps += 1\n",
    "        end\n",
    "    end\n",
    "    push!(mystrides,nsteps)\n",
    "    return(mybasis,mystrides)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mybasis,mystrides) = gd_support(coarse_normdw);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(mystrides) # this is the effective dimension of the gd trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the gd steps were random vectors what would be the calculated dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dw = [ randn(length(coarse_dw[1])) for i=1:floor(Int,(Nepoch-1)/100)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_normdw = [ v/norm(v) for v in rand_dw];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(randbasis,randstrides) = gd_support(rand_normdw);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(randstrides) # effective dimension of the \"random\" gd steps (same number of steps, same vector size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
